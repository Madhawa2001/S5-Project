{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "601c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a62be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.read_csv('../Dataset/DEMO.csv')\n",
    "pbcd_df = pd.read_csv('../Dataset/PBCD_I.csv')\n",
    "rhq_df = pd.read_csv('../Dataset/RHQ_I.csv')\n",
    "tst_df = pd.read_csv('../Dataset/TST_I.csv')\n",
    "bmx_df = pd.read_csv('../Dataset/BMX_I.csv')\n",
    "ihgem_df = pd.read_csv('../Dataset/IHGEM_I.csv')\n",
    "\n",
    "for df in [demo_df, pbcd_df, rhq_df, tst_df]:\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fe984860",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = {\n",
    "    'SEQN': 'Respondent sequence number', # [cite: 498, 500]\n",
    "    'WTSH2YR': 'Blood metal weights', # [cite: 506]\n",
    "    'LBXBPB': 'Blood lead (ug/dL)', # [cite: 512]\n",
    "    'LBDBPBSI': 'Blood lead (umol/L)', # [cite: 519, 521]\n",
    "    'LBDBPBLC': 'Blood lead comment code', # [cite: 525]\n",
    "    'LBXBCD': 'Blood cadmium (ug/L)', # [cite: 527]\n",
    "    'LBDBCDSI': 'Blood cadmium (umol/L)', # [cite: 529]\n",
    "    'LBDBCDLC': 'Blood cadmium comment code', # [cite: 531]\n",
    "    'LBXTHG': 'Blood mercury, total (ug/L)', # [cite: 533]\n",
    "    'LBDTHGSI': 'Blood mercury, total (nmol/L)', # [cite: 535]\n",
    "    'LBDTHGLC': 'Blood mercury, total comment code', # [cite: 537]\n",
    "    'LBXBSE': 'Blood selenium (ug/L)', # [cite: 539]\n",
    "    'LBDBSESI': 'Blood selenium (umol/L)', # [cite: 541]\n",
    "    'LBDBSELC': 'Blood selenium comment code', # [cite: 543]\n",
    "    'LBXBMN': 'Blood manganese (ug/L)', # [cite: 545]\n",
    "    'LBDBMNSI': 'Blood manganese (umol/L)', # [cite: 547]\n",
    "    'LBDBMNLC': 'Blood manganese comment code', # [cite: 549]\n",
    "    'LBXTST': 'Testosterone, total (ng/dL)', # [cite: 680, 681]\n",
    "    'LBDTSTLC': 'Testosterone comment code', # [cite: 689]\n",
    "    'LBXEST': 'Estradiol (pg/mL)', # [cite: 695, 697]\n",
    "    'LBDESTLC': 'Estradiol Comment Code', # [cite: 701]\n",
    "    'LBXSHBG': 'Sex Hormone Binding Globulin (SHBG, nmol/L)', # [cite: 703]\n",
    "    'LBDSHGLC': 'SHBG Comment Code', # [cite: 705]\n",
    "    'SDDSRVYR': 'Data release cycle',\n",
    "    'RIDSTATR': 'Interview/Examination status',\n",
    "    'RIAGENDR': 'Gender',\n",
    "    'RIDAGEYR': 'Age in years at screening',\n",
    "    'RIDAGEMN': 'Age in months at screening - 0 to 24 mos',\n",
    "    'RIDRETH1': 'Race/Hispanic origin',\n",
    "    'RIDRETH3': 'Race/Hispanic origin w/ NH Asian',\n",
    "    'RIDEXMON': 'Six month time period',\n",
    "    'RIDEXAGM': 'Age in months at exam - 0 to 19 years',\n",
    "    'DMQMILIZ': 'Served active duty in US Armed Forces',\n",
    "    'DMQADFC': 'Served in a foreign country',\n",
    "    'DMDBORN4': 'Country of birth',\n",
    "    'DMDCITZN': 'Citizenship status',\n",
    "    'DMDYRSUS': 'Length of time in US',\n",
    "    'DMDEDUC3': 'Education level - Children/Youth 6-19',\n",
    "    'DMDEDUC2': 'Education level - Adults 20+',\n",
    "    'DMDMARTL': 'Marital status',\n",
    "    'RIDEXPRG': 'Pregnancy status at exam',\n",
    "    'SIALANG': 'Language of SP Interview',\n",
    "    'SIAPROXY': 'Proxy used in SP Interview?',\n",
    "    'SIAINTRP': 'Interpreter used in SP Interview?',\n",
    "    'FIALANG': 'Language of Family Interview',\n",
    "    'FIAPROXY': 'Proxy used in Family Interview?',\n",
    "    'FIAINTRP': 'Interpreter used in Family Interview?',\n",
    "    'MIALANG': 'Language of MEC Interview',\n",
    "    'MIAPROXY': 'Proxy used in MEC Interview?',\n",
    "    'MIAINTRP': 'Interpreter used in MEC Interview?',\n",
    "    'AIALANGA': 'Language of ACASI Interview',\n",
    "    'DMDHHSIZ': 'Total number of people in the Household',\n",
    "    'DMDFMSIZ': 'Total number of people in the Family',\n",
    "    'DMDHHSZA': '# of children 5 years or younger in HH',\n",
    "    'DMDHHSZB': '# of children 6-17 years old in HH',\n",
    "    'DMDHHSZE': '# of adults 60 years or older in HH',\n",
    "    'DMDHRGND': \"HH ref person's gender\",\n",
    "    'DMDHRAGE': \"HH ref person's age in years\",\n",
    "    'DMDHRBR4': \"HH ref person's country of birth\",\n",
    "    'DMDHREDU': \"HH ref person's education level\",\n",
    "    'DMDHRMAR': \"HH ref person's marital status\",\n",
    "    'DMDHSEDU': \"HH ref person's spouse's education level\",\n",
    "    'WTINT2YR': 'Full sample 2 year interview weight',\n",
    "    'WTMEC2YR': 'Full sample 2 year MEC exam weight',\n",
    "    'SDMVPSU': 'Masked variance pseudo-PSU',\n",
    "    'SDMVSTRA': 'Masked variance pseudo-stratum',\n",
    "    'INDHHIN2': 'Annual household income',\n",
    "    'INDFMIN2': 'Annual family income',\n",
    "    'INDFMPIR': 'Ratio of family income to poverty',\n",
    "    'RHQ010': 'Age when first menstrual period occurred',\n",
    "    'RHQ020': 'Age range at first menstrual period',\n",
    "    'RHQ031': 'Had regular periods in past 12 months',\n",
    "    'RHD043': 'Reason not having regular periods',\n",
    "    'RHQ060': 'Age at last menstrual period',\n",
    "    'RHQ070': 'Age range at last menstrual period',\n",
    "    'RHQ074': 'Tried for a year to become pregnant?',\n",
    "    'RHQ076': 'Seen a DR b/c unable to become pregnant?',\n",
    "    'RHQ078': 'Ever treated for a pelvic infection/PID?',\n",
    "    'RHQ131': 'Ever been pregnant?',\n",
    "    'RHD143': 'Are you pregnant now?',\n",
    "    'RHQ160': 'How many times have been pregnant?',\n",
    "    'RHQ162': 'During pregnancy, told you have diabetes',\n",
    "    'RHQ163': 'Age told you had diabetes while pregnant',\n",
    "    'RHQ166': 'How many vaginal deliveries?',\n",
    "    'RHQ169': 'How many cesarean deliveries?',\n",
    "    'RHQ172': 'Any babies weigh 9 lbs or more?',\n",
    "    'RHD173': 'Age when delivered baby 9 lbs or more?',\n",
    "    'RHQ171': 'How many deliveries live birth result?',\n",
    "    'RHD180': 'Age at first live birth',\n",
    "    'RHD190': 'Age at last live birth',\n",
    "    'RHQ197': 'How many months ago have baby?',\n",
    "    'RHQ200': 'Now breastfeeding a child?',\n",
    "    'RHD280': 'Had a hysterectomy?',\n",
    "    'RHQ291': 'Age when had hysterectomy',\n",
    "    'RHQ305': 'Had both ovaries removed?',\n",
    "    'RHQ332': 'Age when both ovaries removed',\n",
    "    'RHQ420': 'Ever taken birth control pills?',\n",
    "    'RHQ540': 'Ever use female hormones?',\n",
    "    'RHQ542A': 'Hormone pills used',\n",
    "    'RHQ542B': 'Hormone patches used',\n",
    "    'RHQ542C': 'Hormone cream/suppository/injection used',\n",
    "    'RHQ542D': 'Other form of female hormone used',\n",
    "    'RHQ554': 'Use hormone pills w/estrogen only',\n",
    "    'RHQ560Q': 'How long taking estrogen-only pills?',\n",
    "    'RHQ560U': 'Unit of measure: months, years',\n",
    "    'RHQ570': 'Used estrogen/progestin combo pills',\n",
    "    'RHQ576Q': 'How long taking estrogen/progestin?',\n",
    "    'RHQ576U': 'Unit of measure: months, years',\n",
    "    'RHQ580': 'Used estrogen-only patches?',\n",
    "    'RHQ586Q': 'How long using estrogen only patches?',\n",
    "    'RHQ586U': 'Unit of measure: months, years',\n",
    "    'RHQ596': 'Used estrogen/progestin combo patches?',\n",
    "    'RHQ602Q': 'How long use estrogen/progestin patch',\n",
    "    'RHQ602U': 'Unit of measure: months, years'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd29e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_desc = {}\n",
    "\n",
    "def print_desc():\n",
    "    for key, value in df_desc.items():\n",
    "        print(f\"df_{key}: {value}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "def plot_df(df, columns=None, plot_type='hist', target=None, hue=None, bins=30,\n",
    "            clip_outliers=True, standardize=False, save_path=None,\n",
    "            method='pearson', annot=True):\n",
    "    \"\"\"\n",
    "    Enhanced plotting utility with heatmap support.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "    - columns: list of str — column names to plot (1 or 2 depending on plot)\n",
    "    - plot_type: str — 'hist', 'box', 'violin', 'scatter', 'pairplot', 'heatmap'\n",
    "    - target: str — required for scatter plot\n",
    "    - hue: str — optional group/categorical column\n",
    "    - bins: int — histogram bin count\n",
    "    - clip_outliers: bool — whether to remove outliers (1st–99th percentile)\n",
    "    - standardize: bool — apply z-score normalization\n",
    "    - save_path: str — if provided, saves plots to this directory\n",
    "    - method: str — correlation method for heatmap ('pearson', 'spearman', 'kendall')\n",
    "    - annot: bool — annotate heatmap cells with values\n",
    "    \"\"\"\n",
    "    \n",
    "    def _clip_series(series):\n",
    "        if clip_outliers:\n",
    "            lower, upper = series.quantile(0.01), series.quantile(0.99)\n",
    "            return series.clip(lower, upper)\n",
    "        return series\n",
    "\n",
    "    def _standardize_series(series):\n",
    "        return (series - series.mean()) / series.std() if standardize else series\n",
    "\n",
    "    def _plot_and_save(fig, name):\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            fig_path = os.path.join(save_path, f\"{name}_{plot_type}.png\")\n",
    "            plt.savefig(fig_path, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    # HEATMAP\n",
    "    if plot_type == 'heatmap':\n",
    "        if columns is None:\n",
    "            columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "        corr_df = df[columns].copy()\n",
    "        for col in columns:\n",
    "            corr_df[col] = _clip_series(corr_df[col])\n",
    "            corr_df[col] = _standardize_series(corr_df[col])\n",
    "\n",
    "        corr = corr_df.corr(method=method)\n",
    "        fig = plt.figure(figsize=(len(columns), len(columns)))\n",
    "        sns.heatmap(corr, annot=annot, fmt=\".2f\", cmap=\"coolwarm\", square=True, cbar_kws={'label': f'{method.capitalize()} Correlation'})\n",
    "        plt.title(f'{method.capitalize()} Correlation Heatmap')\n",
    "        _plot_and_save(fig, \"correlation\")\n",
    "\n",
    "        return  # early return\n",
    "\n",
    "    # OTHER PLOTS\n",
    "    if plot_type == 'hist':\n",
    "        for col in columns:\n",
    "            series = df[col].dropna()\n",
    "            series = _clip_series(series)\n",
    "            series = _standardize_series(series)\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 4))\n",
    "            if hue is not None and hue in df.columns:\n",
    "                # Align hue values with the filtered series index\n",
    "                hue_series = df.loc[series.index, hue]\n",
    "                sns.histplot(\n",
    "                    x=series,\n",
    "                    hue=hue_series,\n",
    "                    bins=bins,\n",
    "                    kde=True,\n",
    "                    palette=\"Set2\"\n",
    "                )\n",
    "            else:\n",
    "                sns.histplot(series, bins=bins, kde=True)\n",
    "\n",
    "            plt.title(f'Histogram of {col}')\n",
    "            plt.xlabel(f'{col} {\"(z-score)\" if standardize else \"\"}')\n",
    "            plt.ylabel('Count')\n",
    "            _plot_and_save(fig, col)\n",
    "\n",
    "    elif plot_type == 'box':\n",
    "        for col in columns:\n",
    "            fig = plt.figure(figsize=(8, 4))\n",
    "            temp_df = df[[col] + ([hue] if hue else [])].dropna()\n",
    "            temp_df[col] = _clip_series(temp_df[col])\n",
    "            temp_df[col] = _standardize_series(temp_df[col])\n",
    "\n",
    "            sns.boxplot(data=temp_df, x=hue, y=col) if hue else sns.boxplot(y=temp_df[col])\n",
    "            plt.title(f'Box Plot of {col}')\n",
    "            _plot_and_save(fig, col)\n",
    "\n",
    "    elif plot_type == 'violin':\n",
    "        for col in columns:\n",
    "            fig = plt.figure(figsize=(8, 4))\n",
    "            temp_df = df[[col] + ([hue] if hue else [])].dropna()\n",
    "            temp_df[col] = _clip_series(temp_df[col])\n",
    "            temp_df[col] = _standardize_series(temp_df[col])\n",
    "\n",
    "            sns.violinplot(data=temp_df, x=hue, y=col) if hue else sns.violinplot(y=temp_df[col])\n",
    "            plt.title(f'Violin Plot of {col}')\n",
    "            _plot_and_save(fig, col)\n",
    "\n",
    "    elif plot_type == 'scatter':\n",
    "        if len(columns) != 1 or target is None:\n",
    "            print(\"Error: For scatter, pass one feature column in `columns` and a target in `target`.\")\n",
    "            return\n",
    "        x = df[columns[0]]\n",
    "        y = df[target]\n",
    "        temp_df = df[[columns[0], target] + ([hue] if hue else [])].dropna()\n",
    "        temp_df[columns[0]] = _clip_series(temp_df[columns[0]])\n",
    "        temp_df[target] = _clip_series(temp_df[target])\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(data=temp_df, x=columns[0], y=target, hue=hue, alpha=0.7, edgecolor=None)\n",
    "        plt.title(f'{columns[0]} vs {target}')\n",
    "        _plot_and_save(fig, f\"{columns[0]}_vs_{target}\")\n",
    "\n",
    "    elif plot_type == 'pairplot':\n",
    "        if len(columns) < 2:\n",
    "            print(\"Error: Pairplot needs at least 2 columns.\")\n",
    "            return\n",
    "        cols = columns + ([hue] if hue else [])\n",
    "        temp_df = df[cols].dropna()\n",
    "        for col in columns:\n",
    "            temp_df[col] = _clip_series(temp_df[col])\n",
    "            temp_df[col] = _standardize_series(temp_df[col])\n",
    "        sns.pairplot(temp_df, hue=hue)\n",
    "        if save_path:\n",
    "            plt.savefig(os.path.join(save_path, \"pairplot.png\"), dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported plot_type: {plot_type}. Use 'hist', 'box', 'violin', 'scatter', 'pairplot', or 'heatmap'.\")\n",
    "\n",
    "def get_columns_with_missing_data(df, threshold=0.4):\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    cols_above_threshold = missing_ratio[missing_ratio > threshold]\n",
    "    return cols_above_threshold.sort_values(ascending=False)\n",
    "\n",
    "def get_columns_with_missing_values(df, threshold=0.5):\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    # Filter column names with missing ratio > threshold\n",
    "    cols = missing_ratio[missing_ratio > threshold].index.tolist()\n",
    "    return cols\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def save_df_to_csv(df, name, directory='csvs'):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    # Build full file path\n",
    "    file_path = os.path.join(directory, f\"{name}.csv\")\n",
    "    # Save to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"✅ DataFrame saved to: {file_path}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_feature_importance(\n",
    "    df, \n",
    "    target_col, \n",
    "    drop_cols=None, \n",
    "    model_type='regression', \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    top_n=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model and plot top N feature importances.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - target_col: column name to predict\n",
    "    - drop_cols: list of columns to exclude from features (e.g. ID or other targets)\n",
    "    - model_type: 'regression' or 'classification'\n",
    "    - test_size: fraction of data for test split\n",
    "    - random_state: random seed for reproducibility\n",
    "    - top_n: number of top features to plot\n",
    "    \"\"\"\n",
    "\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "\n",
    "    global top_features\n",
    "    top_features = []\n",
    "\n",
    "    # Prepare X and y\n",
    "    features = df.drop([target_col] + drop_cols, axis=1)\n",
    "    target = df[target_col]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Choose model\n",
    "    if model_type == 'regression':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "    elif model_type == 'classification':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'regression' or 'classification'\")\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Feature importances\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    if top_n > len(indices):\n",
    "        top_n = len(indices)\n",
    "    \n",
    "    # Limit to top_n features\n",
    "    top_indices = indices[:top_n]\n",
    "    top_features = [features.columns[i] for i in top_indices]\n",
    "    top_importances = importances[top_indices]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Top {top_n} Feature Importances - Target: {target_col}\")\n",
    "    plt.bar(range(top_n), top_importances, align=\"center\")\n",
    "    plt.xticks(range(top_n), top_features, rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "def preprocess_and_model(df, target_col, drop_cols=None, test_size=0.2, random_state=42, top_features=60):\n",
    "    \"\"\"\n",
    "    df: input dataframe with target column included\n",
    "    target_col: name of the target column as string\n",
    "    drop_cols: list of column names to drop before modeling\n",
    "    test_size: fraction for test split\n",
    "    random_state: for reproducibility\n",
    "    top_features: number of features to select with RandomForest\n",
    "\n",
    "    Returns dictionary with performance metrics and best params of models\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop ignored columns\n",
    "    if drop_cols:\n",
    "        drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "        df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Identify columns by data type and unique values\n",
    "    yes_no_cols = [col for col in X.columns if set(X[col].dropna().unique()).issubset({'yes', 'no', 'Yes', 'No', 'YES', 'NO'})]\n",
    "    cat_cols_oh = [col for col in X.columns if X[col].nunique() < 5 and col not in yes_no_cols and X[col].dtype == 'object']\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    high_card_cols = [col for col in numeric_cols if X[col].nunique() > 30]\n",
    "    low_card_num_cols = [col for col in numeric_cols if X[col].nunique() <= 30]\n",
    "\n",
    "    # Imputers\n",
    "    yes_no_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Process Yes/No columns\n",
    "    if yes_no_cols:\n",
    "        X_yes_no_imp = yes_no_imputer.fit_transform(X[yes_no_cols])\n",
    "        X_yes_no = pd.DataFrame(X_yes_no_imp, columns=yes_no_cols)\n",
    "        for col in yes_no_cols:\n",
    "            X_yes_no[col] = X_yes_no[col].str.lower().map({'yes': 1, 'no': 0})\n",
    "    else:\n",
    "        X_yes_no = pd.DataFrame()\n",
    "\n",
    "    # Process categorical columns (<5 unique, excluding yes/no)\n",
    "    if cat_cols_oh:\n",
    "        X_cat_imp = cat_imputer.fit_transform(X[cat_cols_oh])\n",
    "        X_cat = pd.DataFrame(X_cat_imp, columns=cat_cols_oh)\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_cat_ohe = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names_out(cat_cols_oh))\n",
    "    else:\n",
    "        X_cat_ohe = pd.DataFrame()\n",
    "\n",
    "    # High-cardinality numeric (scale)\n",
    "    if high_card_cols:\n",
    "        X_high_card_imp = num_imputer.fit_transform(X[high_card_cols])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_high_card = pd.DataFrame(scaler.fit_transform(X_high_card_imp), columns=high_card_cols)\n",
    "    else:\n",
    "        X_high_card = pd.DataFrame()\n",
    "\n",
    "    # Low-cardinality numeric (no scale)\n",
    "    if low_card_num_cols:\n",
    "        X_low_card_imp = num_imputer.fit_transform(X[low_card_num_cols])\n",
    "        X_low_card_num = pd.DataFrame(X_low_card_imp, columns=low_card_num_cols)\n",
    "    else:\n",
    "        X_low_card_num = pd.DataFrame()\n",
    "\n",
    "    # Combine processed data\n",
    "    X_processed = pd.concat([X_yes_no, X_cat_ohe, X_high_card, X_low_card_num], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Feature selection using RandomForest\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "    X_train_sel = X_train[top_feats]\n",
    "    X_test_sel = X_test[top_feats]\n",
    "\n",
    "    # Models\n",
    "    xgb = XGBRegressor(random_state=random_state, objective='reg:squarederror', eval_metric='mae')\n",
    "    huber = HuberRegressor()\n",
    "\n",
    "    xgb_param_grid = {'n_estimators': [50,100, 500, 1000], 'max_depth': [3, 5, 7, 12], 'learning_rate': [0.05, 0.1, 0.001, 0.01]}\n",
    "    huber_param_grid = {'epsilon': [1.1, 1.35, 1.5], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "    xgb_gs = GridSearchCV(xgb, xgb_param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    huber_gs = GridSearchCV(huber, huber_param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "    xgb_gs.fit(X_train_sel, y_train)\n",
    "    huber_gs.fit(X_train_sel, y_train)\n",
    "\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    results = {\n",
    "        'XGBoost Regressor': {'Best Params': xgb_gs.best_params_, 'Performance': evaluate_model(xgb_gs.best_estimator_, X_test_sel, y_test)},\n",
    "        'Huber Regressor': {'Best Params': huber_gs.best_params_, 'Performance': evaluate_model(huber_gs.best_estimator_, X_test_sel, y_test)},\n",
    "        'Selected Features': [f'{feature} : {columns_names[feature]}' for feature in top_feats],\n",
    "        #'Selected Features': top_feats,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Global results tracker\n",
    "global_results_df = pd.DataFrame(columns=[\"Model\", \"Best Params\", \"MAE\", \"MSE\", \"RMSE\", \"R2\", \"Selected Features\"])\n",
    "\n",
    "def preprocess_and_model(\n",
    "    df,\n",
    "    target_col,\n",
    "    model,\n",
    "    param_grid,\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    top_features=60\n",
    "):\n",
    "    \"\"\"\n",
    "    df: DataFrame with target\n",
    "    target_col: target column name\n",
    "    model: scikit-learn compatible model (e.g. XGBRegressor())\n",
    "    param_grid: dict for GridSearchCV\n",
    "    drop_cols: list of columns to drop before processing\n",
    "    \"\"\"\n",
    "    global global_results_df  # so we can append results\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if drop_cols:\n",
    "        drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "        df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Detect column types\n",
    "    yes_no_cols = [col for col in X.columns if set(X[col].dropna().unique()).issubset({'yes','no','Yes','No','YES','NO'})]\n",
    "    cat_cols_oh = [col for col in X.columns if X[col].nunique() < 5 and col not in yes_no_cols and X[col].dtype == 'object']\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    high_card_cols = [col for col in numeric_cols if X[col].nunique() > 30]\n",
    "    low_card_num_cols = [col for col in numeric_cols if X[col].nunique() <= 30]\n",
    "\n",
    "    yes_no_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Process Yes/No\n",
    "    if yes_no_cols:\n",
    "        X_yes_no_imp = yes_no_imputer.fit_transform(X[yes_no_cols])\n",
    "        X_yes_no = pd.DataFrame(X_yes_no_imp, columns=yes_no_cols)\n",
    "        for col in yes_no_cols:\n",
    "            X_yes_no[col] = X_yes_no[col].str.lower().map({'yes': 1, 'no': 0})\n",
    "    else:\n",
    "        X_yes_no = pd.DataFrame()\n",
    "\n",
    "    # Process categorical\n",
    "    if cat_cols_oh:\n",
    "        X_cat_imp = cat_imputer.fit_transform(X[cat_cols_oh])\n",
    "        X_cat = pd.DataFrame(X_cat_imp, columns=cat_cols_oh)\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_cat_ohe = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names_out(cat_cols_oh))\n",
    "    else:\n",
    "        X_cat_ohe = pd.DataFrame()\n",
    "\n",
    "    # High-card numeric\n",
    "    if high_card_cols:\n",
    "        X_high_card_imp = num_imputer.fit_transform(X[high_card_cols])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_high_card = pd.DataFrame(scaler.fit_transform(X_high_card_imp), columns=high_card_cols)\n",
    "    else:\n",
    "        X_high_card = pd.DataFrame()\n",
    "\n",
    "    # Low-card numeric\n",
    "    if low_card_num_cols:\n",
    "        X_low_card_imp = num_imputer.fit_transform(X[low_card_num_cols])\n",
    "        X_low_card_num = pd.DataFrame(X_low_card_imp, columns=low_card_num_cols)\n",
    "    else:\n",
    "        X_low_card_num = pd.DataFrame()\n",
    "\n",
    "    # Merge all\n",
    "    X_processed = pd.concat([X_yes_no, X_cat_ohe, X_high_card, X_low_card_num], axis=1)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Feature selection\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "    X_train_sel = X_train[top_feats]\n",
    "    X_test_sel = X_test[top_feats]\n",
    "\n",
    "    # GridSearch on given model\n",
    "    gs = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    gs.fit(X_train_sel, y_train)\n",
    "\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    eval_results = evaluate_model(gs.best_estimator_, X_test_sel, y_test)\n",
    "\n",
    "    # Create current results row\n",
    "    current_result = pd.DataFrame([{\n",
    "        \"Model\": model.__class__.__name__,\n",
    "        \"Best Params\": gs.best_params_,\n",
    "        \"MAE\": eval_results[\"MAE\"],\n",
    "        \"MSE\": eval_results[\"MSE\"],\n",
    "        \"RMSE\": eval_results[\"RMSE\"],\n",
    "        \"R2\": eval_results[\"R2\"],\n",
    "        \"Selected Features\": top_feats\n",
    "    }])\n",
    "\n",
    "    # Append to global results\n",
    "    global_results_df = pd.concat([global_results_df, current_result], ignore_index=True)\n",
    "\n",
    "    return current_result\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Folder to save SHAP plots\n",
    "shap_output_dir = \"shap_outputs\"\n",
    "os.makedirs(shap_output_dir, exist_ok=True)\n",
    "\n",
    "# def preprocess_and_model_shap(\n",
    "#     df,\n",
    "#     target_col,\n",
    "#     model,\n",
    "#     param_grid,\n",
    "#     drop_cols=None,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     top_features=60,\n",
    "#     run_shap=True\n",
    "# ):\n",
    "#     global global_results_df\n",
    "\n",
    "#     df = df.copy()\n",
    "\n",
    "#     if drop_cols:\n",
    "#         drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "#         df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "\n",
    "#     yes_no_cols = [\n",
    "#         col for col in X.columns\n",
    "#         if set(X[col].dropna().unique()).issubset({'yes','no','Yes','No','YES','NO'})\n",
    "#     ]\n",
    "#     cat_cols_oh = [\n",
    "#         col for col in X.columns\n",
    "#         if X[col].nunique() < 5 and col not in yes_no_cols and X[col].dtype == 'object'\n",
    "#     ]\n",
    "#     numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "#     high_card_cols = [col for col in numeric_cols if X[col].nunique() > 30]\n",
    "#     low_card_num_cols = [col for col in numeric_cols if X[col].nunique() <= 30]\n",
    "\n",
    "#     yes_no_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#     cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#     num_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "#     if yes_no_cols:\n",
    "#         X_yes_no_imp = yes_no_imputer.fit_transform(X[yes_no_cols])\n",
    "#         X_yes_no = pd.DataFrame(X_yes_no_imp, columns=yes_no_cols)\n",
    "#         for col in yes_no_cols:\n",
    "#             X_yes_no[col] = X_yes_no[col].str.lower().map({'yes': 1, 'no': 0})\n",
    "#     else:\n",
    "#         X_yes_no = pd.DataFrame()\n",
    "\n",
    "#     if cat_cols_oh:\n",
    "#         X_cat_imp = cat_imputer.fit_transform(X[cat_cols_oh])\n",
    "#         X_cat = pd.DataFrame(X_cat_imp, columns=cat_cols_oh)\n",
    "#         ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "#         X_cat_ohe = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names_out(cat_cols_oh))\n",
    "#     else:\n",
    "#         X_cat_ohe = pd.DataFrame()\n",
    "\n",
    "#     if high_card_cols:\n",
    "#         X_high_card_imp = num_imputer.fit_transform(X[high_card_cols])\n",
    "#         scaler = MinMaxScaler()\n",
    "#         X_high_card = pd.DataFrame(scaler.fit_transform(X_high_card_imp), columns=high_card_cols)\n",
    "#     else:\n",
    "#         X_high_card = pd.DataFrame()\n",
    "\n",
    "#     if low_card_num_cols:\n",
    "#         X_low_card_imp = num_imputer.fit_transform(X[low_card_num_cols])\n",
    "#         X_low_card_num = pd.DataFrame(X_low_card_imp, columns=low_card_num_cols)\n",
    "#     else:\n",
    "#         X_low_card_num = pd.DataFrame()\n",
    "\n",
    "#     X_processed = pd.concat([X_yes_no, X_cat_ohe, X_high_card, X_low_card_num], axis=1)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_processed, y, test_size=test_size, random_state=random_state\n",
    "#     )\n",
    "\n",
    "#     rf = RandomForestRegressor(random_state=random_state)\n",
    "#     rf.fit(X_train, y_train)\n",
    "#     importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "#     top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "#     X_train_sel = X_train[top_feats]\n",
    "#     X_test_sel = X_test[top_feats]\n",
    "\n",
    "#     gs = GridSearchCV(model, param_grid, cv=3,\n",
    "#                       scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "#     gs.fit(X_train_sel, y_train)\n",
    "\n",
    "#     def evaluate_model(model, X_test, y_test):\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         return {\n",
    "#             'MAE': mean_absolute_error(y_test, y_pred),\n",
    "#             'MSE': mean_squared_error(y_test, y_pred),\n",
    "#             'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "#             'R2': r2_score(y_test, y_pred)\n",
    "#         }\n",
    "\n",
    "#     eval_results = evaluate_model(gs.best_estimator_, X_test_sel, y_test)\n",
    "\n",
    "#     current_result = pd.DataFrame([{\n",
    "#         \"Model\": model.__class__.__name__,\n",
    "#         \"Best Params\": gs.best_params_,\n",
    "#         \"MAE\": eval_results[\"MAE\"],\n",
    "#         \"MSE\": eval_results[\"MSE\"],\n",
    "#         \"RMSE\": eval_results[\"RMSE\"],\n",
    "#         \"R2\": eval_results[\"R2\"],\n",
    "#         \"Selected Features\": top_feats\n",
    "#     }])\n",
    "\n",
    "#     global_results_df = pd.concat([global_results_df, current_result], ignore_index=True)\n",
    "\n",
    "#     # ---------- SHAP Analysis ----------\n",
    "#     if run_shap:\n",
    "#         explainer = shap.Explainer(gs.best_estimator_, X_train_sel)\n",
    "#         shap_values = explainer(X_test_sel)\n",
    "\n",
    "#         # Summary plot (global feature importance)\n",
    "#         plt.figure()\n",
    "#         shap.summary_plot(shap_values, X_test_sel, show=False)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_summary.png\"))\n",
    "#         plt.close()\n",
    "\n",
    "#         # Bar plot\n",
    "#         plt.figure()\n",
    "#         shap.summary_plot(shap_values, X_test_sel, plot_type=\"bar\", show=False)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_bar.png\"))\n",
    "#         plt.close()\n",
    "\n",
    "#         # Example individual prediction waterfall\n",
    "#         shap.plots.waterfall(shap_values[0], show=False)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_waterfall.png\"))\n",
    "#         plt.close()\n",
    "\n",
    "#     return current_result, gs.best_estimator_, X_test_sel, top_feats, shap_values\n",
    "\n",
    "def preprocess_and_model_shap(\n",
    "    df,\n",
    "    target_col,\n",
    "    model,\n",
    "    param_grid,\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    top_features=60,\n",
    "    run_shap=True,\n",
    "    external_results_path=\"all_results.csv\"\n",
    "):\n",
    "    global global_results_df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop unwanted columns if present\n",
    "    if drop_cols:\n",
    "        drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "        df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "    # Split features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # ---------------- Data preprocessing ----------------\n",
    "    yes_no_cols = [\n",
    "        col for col in X.columns\n",
    "        if set(X[col].dropna().unique()).issubset({'yes','no','Yes','No','YES','NO'})\n",
    "    ]\n",
    "    cat_cols_oh = [\n",
    "        col for col in X.columns\n",
    "        if X[col].nunique() < 5 and col not in yes_no_cols and X[col].dtype == 'object'\n",
    "    ]\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    high_card_cols = [col for col in numeric_cols if X[col].nunique() > 30]\n",
    "    low_card_num_cols = [col for col in numeric_cols if X[col].nunique() <= 30]\n",
    "\n",
    "    yes_no_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    if yes_no_cols:\n",
    "        X_yes_no_imp = yes_no_imputer.fit_transform(X[yes_no_cols])\n",
    "        X_yes_no = pd.DataFrame(X_yes_no_imp, columns=yes_no_cols)\n",
    "        for col in yes_no_cols:\n",
    "            X_yes_no[col] = X_yes_no[col].str.lower().map({'yes': 1, 'no': 0})\n",
    "    else:\n",
    "        X_yes_no = pd.DataFrame()\n",
    "\n",
    "    if cat_cols_oh:\n",
    "        X_cat_imp = cat_imputer.fit_transform(X[cat_cols_oh])\n",
    "        X_cat = pd.DataFrame(X_cat_imp, columns=cat_cols_oh)\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_cat_ohe = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names_out(cat_cols_oh))\n",
    "    else:\n",
    "        X_cat_ohe = pd.DataFrame()\n",
    "\n",
    "    if high_card_cols:\n",
    "        X_high_card_imp = num_imputer.fit_transform(X[high_card_cols])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_high_card = pd.DataFrame(scaler.fit_transform(X_high_card_imp), columns=high_card_cols)\n",
    "    else:\n",
    "        X_high_card = pd.DataFrame()\n",
    "\n",
    "    if low_card_num_cols:\n",
    "        X_low_card_imp = num_imputer.fit_transform(X[low_card_num_cols])\n",
    "        X_low_card_num = pd.DataFrame(X_low_card_imp, columns=low_card_num_cols)\n",
    "    else:\n",
    "        X_low_card_num = pd.DataFrame()\n",
    "\n",
    "    X_processed = pd.concat([X_yes_no, X_cat_ohe, X_high_card, X_low_card_num], axis=1)\n",
    "\n",
    "    # ---------------- Train-test split ----------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ---------------- Feature selection via RF ----------------\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "    X_train_sel = X_train[top_feats]\n",
    "    X_test_sel = X_test[top_feats]\n",
    "\n",
    "    # ---------------- Model training with GridSearch ----------------\n",
    "    gs = GridSearchCV(model, param_grid, cv=3,\n",
    "                      scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    gs.fit(X_train_sel, y_train)\n",
    "\n",
    "    # ---------------- Evaluation ----------------\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    eval_results = evaluate_model(gs.best_estimator_, X_test_sel, y_test)\n",
    "\n",
    "    # Current result row\n",
    "    current_result = pd.DataFrame([{\n",
    "        \"Model\": model.__class__.__name__,\n",
    "        \"Best Params\": gs.best_params_,\n",
    "        \"MAE\": eval_results[\"MAE\"],\n",
    "        \"MSE\": eval_results[\"MSE\"],\n",
    "        \"RMSE\": eval_results[\"RMSE\"],\n",
    "        \"R2\": eval_results[\"R2\"],\n",
    "        \"Selected Features\": top_feats,\n",
    "        \"Target Variable\": target_col\n",
    "    }])\n",
    "\n",
    "    # Add to global results\n",
    "    global_results_df = pd.concat([global_results_df, current_result], ignore_index=True)\n",
    "\n",
    "    # ---------------- External results persistence ----------------\n",
    "    if os.path.exists(external_results_path):\n",
    "        external_results = pd.read_csv(external_results_path)\n",
    "        external_results = pd.concat([external_results, current_result], ignore_index=True)\n",
    "    else:\n",
    "        external_results = current_result\n",
    "\n",
    "    external_results.to_csv(external_results_path, index=False)\n",
    "\n",
    "    # ---------------- SHAP Analysis ----------------\n",
    "    shap_values = None\n",
    "    if run_shap:\n",
    "        explainer = shap.Explainer(gs.best_estimator_, X_train_sel)\n",
    "        shap_values = explainer(X_test_sel,check_additivity=False)\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_sel, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_summary.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Bar plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_sel, plot_type=\"bar\", show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_bar.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Example waterfall\n",
    "        shap.plots.waterfall(shap_values[0], show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_waterfall.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------- Return structured outputs ----------------\n",
    "    results_dict = {\n",
    "        \"best_model\": gs.best_estimator_,\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"eval_results\": eval_results,\n",
    "        \"X_train\": X_train_sel,\n",
    "        \"X_test\": X_test_sel,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"selected_features\": top_feats,\n",
    "        \"shap_values\": shap_values\n",
    "    }\n",
    "\n",
    "    return current_result, results_dict\n",
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "# global results dataframe\n",
    "global_results_df = pd.DataFrame()\n",
    "\n",
    "# ensure shap output directory exists\n",
    "shap_output_dir = \"shap_outputs\"\n",
    "os.makedirs(shap_output_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_and_model_shap_save(\n",
    "    df,\n",
    "    target_col,\n",
    "    model,\n",
    "    param_grid,\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    top_features=60,\n",
    "    run_shap=True,\n",
    "    external_results_path=\"all_results.csv\",\n",
    "    save_model=False,                      \n",
    "    model_path=\"best_model.joblib\"\n",
    "):\n",
    "    global global_results_df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------------- Drop unwanted columns ----------------\n",
    "    if drop_cols:\n",
    "        drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "        df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "    # ---------------- Split features and target ----------------\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # ---------------- Data preprocessing ----------------\n",
    "    yes_no_cols = [\n",
    "        col for col in X.columns\n",
    "        if set(X[col].dropna().unique()).issubset({'yes','no','Yes','No','YES','NO'})\n",
    "    ]\n",
    "    cat_cols_oh = [\n",
    "        col for col in X.columns\n",
    "        if X[col].nunique() < 5 and col not in yes_no_cols and X[col].dtype == 'object'\n",
    "    ]\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    high_card_cols = [col for col in numeric_cols if X[col].nunique() > 30]\n",
    "    low_card_num_cols = [col for col in numeric_cols if X[col].nunique() <= 30]\n",
    "\n",
    "    yes_no_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    if yes_no_cols:\n",
    "        X_yes_no_imp = yes_no_imputer.fit_transform(X[yes_no_cols])\n",
    "        X_yes_no = pd.DataFrame(X_yes_no_imp, columns=yes_no_cols)\n",
    "        for col in yes_no_cols:\n",
    "            X_yes_no[col] = X_yes_no[col].str.lower().map({'yes': 1, 'no': 0})\n",
    "    else:\n",
    "        X_yes_no = pd.DataFrame()\n",
    "\n",
    "    if cat_cols_oh:\n",
    "        X_cat_imp = cat_imputer.fit_transform(X[cat_cols_oh])\n",
    "        X_cat = pd.DataFrame(X_cat_imp, columns=cat_cols_oh)\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_cat_ohe = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names_out(cat_cols_oh))\n",
    "    else:\n",
    "        X_cat_ohe = pd.DataFrame()\n",
    "\n",
    "    if high_card_cols:\n",
    "        X_high_card_imp = num_imputer.fit_transform(X[high_card_cols])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_high_card = pd.DataFrame(scaler.fit_transform(X_high_card_imp), columns=high_card_cols)\n",
    "    else:\n",
    "        X_high_card = pd.DataFrame()\n",
    "\n",
    "    if low_card_num_cols:\n",
    "        X_low_card_imp = num_imputer.fit_transform(X[low_card_num_cols])\n",
    "        X_low_card_num = pd.DataFrame(X_low_card_imp, columns=low_card_num_cols)\n",
    "    else:\n",
    "        X_low_card_num = pd.DataFrame()\n",
    "\n",
    "    # combine all processed parts\n",
    "    X_processed = pd.concat([X_yes_no, X_cat_ohe, X_high_card, X_low_card_num], axis=1)\n",
    "\n",
    "\n",
    "    preprocessing_objects = {\n",
    "    \"yes_no_imputer\": yes_no_imputer if yes_no_cols else None,\n",
    "    \"cat_imputer\": cat_imputer if cat_cols_oh else None,\n",
    "    \"ohe\": ohe if cat_cols_oh else None,\n",
    "    \"num_imputer\": num_imputer,\n",
    "    \"scaler_high_card\": scaler if high_card_cols else None\n",
    "    }\n",
    "\n",
    "    # ---------------- Train-test split ----------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ---------------- Feature selection via RandomForest ----------------\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "    X_train_sel = X_train[top_feats]\n",
    "    X_test_sel = X_test[top_feats]\n",
    "\n",
    "    # ---------------- Model training with GridSearch ----------------\n",
    "    gs = GridSearchCV(model, param_grid, cv=3,\n",
    "                      scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    gs.fit(X_train_sel, y_train)\n",
    "\n",
    "    # ---------------- Evaluation ----------------\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    eval_results = evaluate_model(gs.best_estimator_, X_test_sel, y_test)\n",
    "\n",
    "    # ---------------- Save best model if requested ----------------\n",
    "    if save_model:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)  # 👈 create path if missing\n",
    "        joblib.dump(gs.best_estimator_, model_path)\n",
    "        print(f\"✅ Best model saved to {model_path}\")\n",
    "\n",
    "\n",
    "    # ---------------- Current result row ----------------\n",
    "    current_result = pd.DataFrame([{\n",
    "        \"Model\": model.__class__.__name__,\n",
    "        \"Best Params\": gs.best_params_,\n",
    "        \"MAE\": eval_results[\"MAE\"],\n",
    "        \"MSE\": eval_results[\"MSE\"],\n",
    "        \"RMSE\": eval_results[\"RMSE\"],\n",
    "        \"R2\": eval_results[\"R2\"],\n",
    "        \"Selected Features\": top_feats,\n",
    "        \"Target Variable\": target_col\n",
    "    }])\n",
    "\n",
    "    # Add to global results\n",
    "    global_results_df = pd.concat([global_results_df, current_result], ignore_index=True)\n",
    "\n",
    "    # ---------------- External results persistence ----------------\n",
    "    if os.path.exists(external_results_path):\n",
    "        external_results = pd.read_csv(external_results_path)\n",
    "        external_results = pd.concat([external_results, current_result], ignore_index=True)\n",
    "    else:\n",
    "        external_results = current_result\n",
    "\n",
    "    external_results.to_csv(external_results_path, index=False)\n",
    "\n",
    "    # ---------------- SHAP Analysis ----------------\n",
    "    shap_values = None\n",
    "    if run_shap:\n",
    "        explainer = shap.Explainer(gs.best_estimator_, X_train_sel)\n",
    "        shap_values = explainer(X_test_sel)\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_sel, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_summary.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Bar plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_sel, plot_type=\"bar\", show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_bar.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Example waterfall\n",
    "        shap.plots.waterfall(shap_values[0], show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_waterfall.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------- Return structured outputs ----------------\n",
    "    results_dict = {\n",
    "        \"best_model\": gs.best_estimator_,\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"eval_results\": eval_results,\n",
    "        \"X_train\": X_train_sel,\n",
    "        \"X_test\": X_test_sel,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"selected_features\": top_feats,\n",
    "        \"shap_values\": shap_values,\n",
    "        \"preprocessing_objects\": preprocessing_objects \n",
    "    }\n",
    "\n",
    "    return current_result, results_dict\n",
    "\n",
    "def preprocess_and_model_shap_save_2(\n",
    "    df,\n",
    "    target_col,\n",
    "    model,\n",
    "    param_grid,\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    top_features=60,\n",
    "    run_shap=True,\n",
    "    external_results_path=\"all_results.csv\",\n",
    "    save_model=False,\n",
    "    model_path=\"best_model.joblib\",\n",
    "    shap_output_dir=\"shap_outputs\"\n",
    "):\n",
    "    global global_results_df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------------- Drop unwanted columns ----------------\n",
    "    if drop_cols:\n",
    "        drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "        df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "    # ---------------- Split features and target ----------------\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # ---------------- Data preprocessing (Simple Mode Imputation Only) ----------------\n",
    "    imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    X_processed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    preprocessing_objects = {\"imputer\": imputer}\n",
    "\n",
    "    # ---------------- Train-test split ----------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ---------------- Feature selection via RandomForest ----------------\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "    X_train_sel = X_train[top_feats]\n",
    "    X_test_sel = X_test[top_feats]\n",
    "\n",
    "    # ---------------- Model training with GridSearch ----------------\n",
    "    gs = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    gs.fit(X_train_sel, y_train)\n",
    "\n",
    "    # ---------------- Evaluation ----------------\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            \"R2\": r2_score(y_test, y_pred),\n",
    "        }\n",
    "\n",
    "    eval_results = evaluate_model(gs.best_estimator_, X_test_sel, y_test)\n",
    "\n",
    "    # ---------------- Save best model if requested ----------------\n",
    "    if save_model:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        joblib.dump(gs.best_estimator_, model_path)\n",
    "        print(f\"✅ Best model saved to {model_path}\")\n",
    "\n",
    "    # ---------------- Current result row ----------------\n",
    "    current_result = pd.DataFrame([{\n",
    "        \"Model\": model.__class__.__name__,\n",
    "        \"Best Params\": gs.best_params_,\n",
    "        \"MAE\": eval_results[\"MAE\"],\n",
    "        \"MSE\": eval_results[\"MSE\"],\n",
    "        \"RMSE\": eval_results[\"RMSE\"],\n",
    "        \"R2\": eval_results[\"R2\"],\n",
    "        \"Selected Features\": top_feats,\n",
    "        \"Target Variable\": target_col,\n",
    "    }])\n",
    "\n",
    "    # Add to global results\n",
    "    global_results_df = pd.concat([global_results_df, current_result], ignore_index=True)\n",
    "\n",
    "    # ---------------- External results persistence ----------------\n",
    "    if os.path.exists(external_results_path):\n",
    "        external_results = pd.read_csv(external_results_path)\n",
    "        external_results = pd.concat([external_results, current_result], ignore_index=True)\n",
    "    else:\n",
    "        external_results = current_result\n",
    "\n",
    "    external_results.to_csv(external_results_path, index=False)\n",
    "\n",
    "    # ---------------- SHAP Analysis ----------------\n",
    "    shap_values = None\n",
    "    if run_shap:\n",
    "        os.makedirs(shap_output_dir, exist_ok=True)\n",
    "        explainer = shap.Explainer(gs.best_estimator_, X_train_sel)\n",
    "        shap_values = explainer(X_test_sel)\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_sel, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_summary.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Bar plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_sel, plot_type=\"bar\", show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_bar.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Example waterfall\n",
    "        shap.plots.waterfall(shap_values[0], show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(shap_output_dir, f\"{model.__class__.__name__}_waterfall.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------- Return structured outputs ----------------\n",
    "    results_dict = {\n",
    "        \"best_model\": gs.best_estimator_,\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"eval_results\": eval_results,\n",
    "        \"X_train\": X_train_sel,\n",
    "        \"X_test\": X_test_sel,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"selected_features\": top_feats,\n",
    "        \"shap_values\": shap_values,\n",
    "        \"preprocessing_objects\": preprocessing_objects,\n",
    "    }\n",
    "\n",
    "    return current_result, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "458fecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = demo_df.copy()\n",
    "df_1 = df_1.merge(tst_df, on='SEQN', how='right')\n",
    "df_desc[1] = \"demo and tst merged\"\n",
    "\n",
    "df_2 = demo_df.copy()\n",
    "df_2 = df_2.merge(pbcd_df, on='SEQN', how='right')\n",
    "df_desc[2] = \"demo and pbcd merged\"\n",
    "\n",
    "df_3 = demo_df.copy()\n",
    "df_3 = df_3.merge(rhq_df, on='SEQN', how='right')\n",
    "df_desc[3] = \"demo and rhq merged\"\n",
    "\n",
    "df_4 = df_1.copy()\n",
    "df_4 = df_4.merge(pbcd_df, on='SEQN', how='left')\n",
    "df_desc[4] = \"demo and rhq merged + pbcd left merged\"\n",
    "\n",
    "df_5 = df_4.copy()\n",
    "df_5 = df_5.merge(rhq_df, on='SEQN', how='left')\n",
    "df_desc[5] = \"all mergerd for demo, tst, pbcd, rhq order\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0861d7",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5a1e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_skip_patterns(df, skip_rules):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify RHQ/RHD columns in order\n",
    "    rhq_cols = [col for col in df.columns if re.match(r'^RH[QD]\\d{3}[A-Z]*$', col)]\n",
    "    rhq_col_positions = {col: idx for idx, col in enumerate(rhq_cols)}\n",
    "    \n",
    "    pass_id = 1\n",
    "\n",
    "    for trigger_col, skip_map in skip_rules.items():\n",
    "        for trigger_val, skip_to in skip_map.items():\n",
    "            if trigger_col not in rhq_col_positions:\n",
    "                continue\n",
    "\n",
    "            start_idx = rhq_col_positions[trigger_col]\n",
    "            mask = df[trigger_col] == trigger_val\n",
    "\n",
    "            # Determine skip range\n",
    "            if skip_to == \"END\":\n",
    "                skipped_cols = rhq_cols[start_idx + 1:]\n",
    "            else:\n",
    "                if skip_to not in rhq_col_positions:\n",
    "                    continue\n",
    "                end_idx = rhq_col_positions[skip_to]\n",
    "                if end_idx <= start_idx:\n",
    "                    continue\n",
    "                skipped_cols = rhq_cols[start_idx + 1:end_idx]\n",
    "            \n",
    "            # Fill NaNs with pass_k in skipped columns\n",
    "            for col in skipped_cols:\n",
    "                df[col] = df[col].astype('object')  # Ensure column can hold strings\n",
    "                df.loc[mask & df[col].isna(), col] = f\"pass_{pass_id}\"\n",
    "\n",
    "            \n",
    "            pass_id += 1\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f6a6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_rules = {\n",
    "    \"RHQ010\": {\n",
    "        0: \"END\"  \n",
    "    },\n",
    "    \"RHQ031\": {\n",
    "        1: \"RHQ060\",  \n",
    "        7: \"RHQ060\",  \n",
    "        9: \"RHQ060\"   \n",
    "    },\n",
    "    \"RHQ131\": {\n",
    "        2: \"RHD280\",  \n",
    "        7: \"RHD280\",  \n",
    "        9: \"RHD280\"   \n",
    "    },\n",
    "    \n",
    "    \"RHQ162\": {\n",
    "        2: \"RHQ166\",\n",
    "        3: \"RHQ166\",\n",
    "        7: \"RHQ166\",  \n",
    "        9: \"RHQ166\"   \n",
    "    },\n",
    "    \"RHQ172\": {\n",
    "        2: \"RHQ171\",  \n",
    "        7: \"RHQ171\",  \n",
    "        9: \"RHQ171\"\n",
    "    },\n",
    "    \"RHD280\": {\n",
    "        2: \"RHQ305\",  \n",
    "        7: \"RHQ305\",  \n",
    "        9: \"RHQ305\"\n",
    "    },\n",
    "    \"RHQ305\": {\n",
    "        2: \"RHQ420\",  \n",
    "        7: \"RHQ420\",  \n",
    "        9: \"RHQ420\"   \n",
    "    },\n",
    "    \"RHQ540\": {\n",
    "        2: \"END\", \n",
    "        7: \"END\", \n",
    "        9: \"END\"  \n",
    "    },\n",
    "    \"RHQ554\": {\n",
    "        2: \"RHQ570\", \n",
    "        7: \"RHQ570\", \n",
    "        9: \"RHQ570\"  \n",
    "    },\n",
    "    \"RHQ570\": {\n",
    "        2: \"RHQ580\",  \n",
    "        7: \"RHQ580\",  \n",
    "        9: \"RHQ580\" \n",
    "    },\n",
    "    \"RHQ580\": {\n",
    "        2: \"RHQ596\",  \n",
    "        7: \"RHQ596\",\n",
    "        9: \"RHQ596\"   \n",
    "    },\n",
    "    \"RHQ596\": {\n",
    "        2: \"END\",  \n",
    "        7: \"END\",\n",
    "        9: \"END\"   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50462579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = rhq_df.copy()\n",
    "df_6 = apply_skip_patterns(df_6, skip_rules)\n",
    "df_desc[6] = \"Applied skip patterns to RHQ/RHD columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02e0f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_skip_patterns_numeric(df, skip_rules, pass_start=500):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify RHQ/RHD columns in order\n",
    "    rhq_cols = [col for col in df.columns if re.match(r'^RH[QD]\\d{3}[A-Z]*$', col)]\n",
    "    rhq_col_positions = {col: idx for idx, col in enumerate(rhq_cols)}\n",
    "    \n",
    "    pass_id = 1\n",
    "    pass_map = {}  # Store pass_k -> numeric value mapping\n",
    "\n",
    "    for trigger_col, skip_map in skip_rules.items():\n",
    "        for trigger_val, skip_to in skip_map.items():\n",
    "            if trigger_col not in rhq_col_positions:\n",
    "                continue\n",
    "\n",
    "            start_idx = rhq_col_positions[trigger_col]\n",
    "            mask = df[trigger_col] == trigger_val\n",
    "\n",
    "            # Determine skip range\n",
    "            if skip_to == \"END\":\n",
    "                skipped_cols = rhq_cols[start_idx + 1:]\n",
    "            else:\n",
    "                if skip_to not in rhq_col_positions:\n",
    "                    continue\n",
    "                end_idx = rhq_col_positions[skip_to]\n",
    "                if end_idx <= start_idx:\n",
    "                    continue\n",
    "                skipped_cols = rhq_cols[start_idx + 1:end_idx]\n",
    "            \n",
    "            # Generate numeric pass value\n",
    "            pass_label = f\"pass_{pass_id}\"\n",
    "            pass_numeric = pass_start + pass_id - 1\n",
    "            pass_map[pass_label] = pass_numeric\n",
    "\n",
    "            # Apply to relevant columns\n",
    "            for col in skipped_cols:\n",
    "                df.loc[mask & df[col].isna(), col] = pass_label\n",
    "            \n",
    "            pass_id += 1\n",
    "\n",
    "    # Final step: replace all pass_k with numeric codes\n",
    "    df.replace(pass_map, inplace=True)\n",
    "    df = df.infer_objects(copy=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c9611d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\994487393.py:44: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(pass_map, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_7 = df_6.copy()\n",
    "df_7 = apply_skip_patterns_numeric(df_7, skip_rules)\n",
    "df_desc[7] = \"Applied numeric skip patterns to RHQ/RHD columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bdbf4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_male_nans(df, rhq_prefixes=('RHQ', 'RHD'), male_code=300):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify RHQ/RHD columns\n",
    "    rhq_cols = [col for col in df.columns if any(col.startswith(prefix) for prefix in rhq_prefixes)]\n",
    "    \n",
    "    # Mask: rows where gender is male (RIAGENDR == 1)\n",
    "    male_mask = df['RIAGENDR'] == 1\n",
    "    \n",
    "    # Replace NaNs in RHQ columns for male rows with the code\n",
    "    for col in rhq_cols:\n",
    "        df.loc[male_mask & df[col].isna(), col] = male_code\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6ddb0",
   "metadata": {},
   "source": [
    "### CLEANING DEMO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5581ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8 = demo_df.copy()\n",
    "df_desc[8] = \"demo_df copy for df_8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a2f2461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_demo_advanced(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Precompute age and gender filters\n",
    "    age = df['RIDAGEYR']\n",
    "    is_male = df['RIAGENDR'] == 1\n",
    "    is_female = df['RIAGENDR'] == 2\n",
    "\n",
    "    # ----------------------------\n",
    "    # RIDAGEMN: Only valid for age ≤ 2, others get age in months\n",
    "    # ----------------------------\n",
    "    df['RIDAGEMN'] = df.apply(\n",
    "        lambda row: row['RIDAGEMN'] if row['RIDAGEYR'] <= 2 else row['RIDAGEYR'] * 12,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Combine DMDEDUC3 (6–19) and DMDEDUC2 (20+) into a unified education level column\n",
    "    # ----------------------------\n",
    "    df['COMBINED_EDUC'] = df['DMDEDUC3']\n",
    "    # Fill missing values in DMDEDUC3 using 300+DMDEDUC2\n",
    "    mask = df['COMBINED_EDUC'].isna() & df['DMDEDUC2'].notna()\n",
    "    df.loc[mask, 'COMBINED_EDUC'] = df.loc[mask, 'DMDEDUC2'] + 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # DMQMILIZ / DMQADFC: Military service — valid if age ≥ 17\n",
    "    # ----------------------------\n",
    "    df.loc[age < 17, ['DMQMILIZ', 'DMQADFC']] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # DMDEDUC3: Children 6–19 only\n",
    "    # ----------------------------\n",
    "    df.loc[(age < 6) | (age > 19), 'DMDEDUC3'] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # DMDEDUC2: Adults 20+ only\n",
    "    # ----------------------------\n",
    "    df.loc[age < 20, 'DMDEDUC2'] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # DMDMARTL: Marital status — only for 16+ (modified)\n",
    "    # ----------------------------\n",
    "    df.loc[age < 16, 'DMDMARTL'] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # RIDEXPRG: Pregnancy test – only for females age 20–44\n",
    "    # ----------------------------\n",
    "    df.loc[is_male, 'RIDEXPRG'] = 300\n",
    "    df.loc[is_female & (age < 20), 'RIDEXPRG'] = 202\n",
    "    df.loc[is_female & (age > 44), 'RIDEXPRG'] = 203\n",
    "\n",
    "    # ----------------------------\n",
    "    # MEC interview — age ≥ 8\n",
    "    # ----------------------------\n",
    "    df.loc[age < 8, ['MIALANG', 'MIAPROXY', 'MIAINTRP']] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # ACASI Interview: age 8–69\n",
    "    # ----------------------------\n",
    "    df.loc[(age < 8) | (age > 69), 'AIALANGA'] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # SP Interview Language and Proxy: skip if age < 1\n",
    "    # ----------------------------\n",
    "    df.loc[age < 1, ['SIALANG', 'SIAPROXY', 'SIAINTRP']] = 300\n",
    "    df.loc[age < 1, ['FIALANG', 'FIAPROXY', 'FIAINTRP']] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # DMDHSEDU (Spouse’s education): only if married (DMDMARTL == 1)\n",
    "    # ----------------------------\n",
    "    df.loc[df['DMDMARTL'] != 1, 'DMDHSEDU'] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # Household reference fields: assign 300 if missing\n",
    "    # ----------------------------\n",
    "    hh_ref_cols = ['DMDHRGND', 'DMDHRAGE', 'DMDHRBR4', 'DMDHREDU', 'DMDHRMAR']\n",
    "    for col in hh_ref_cols:\n",
    "        df.loc[df[col].isna(), col] = 300\n",
    "\n",
    "    # ----------------------------\n",
    "    # Impute DMDYRSUS using DMDCITZN, DMDBORN4, and RIDAGEYR\n",
    "    # ----------------------------\n",
    "    def age_to_dmdyrsus(years):\n",
    "        if pd.isna(years): return np.nan\n",
    "        if years < 1: return 1\n",
    "        elif years < 5: return 2\n",
    "        elif years < 10: return 3\n",
    "        elif years < 15: return 4\n",
    "        elif years < 20: return 5\n",
    "        elif years < 30: return 6\n",
    "        elif years < 40: return 7\n",
    "        elif years < 50: return 8\n",
    "        else: return 9\n",
    "\n",
    "    mask_na = df['DMDYRSUS'].isna()\n",
    "    born_us = df['DMDBORN4'] == 1\n",
    "    not_born_us = df['DMDBORN4'] == 2\n",
    "    citizen = df['DMDCITZN'] == 1\n",
    "    non_citizen = df['DMDCITZN'] == 2\n",
    "\n",
    "    # Case 1: Born in US → years in US = age\n",
    "    born_us_mask = mask_na & born_us\n",
    "    df.loc[born_us_mask, 'DMDYRSUS'] = age[born_us_mask].apply(age_to_dmdyrsus)\n",
    "\n",
    "    # Case 2: Not born in US but citizen → assume arrived at 5 y/o\n",
    "    citizen_nonborn = mask_na & citizen & not_born_us\n",
    "    df.loc[citizen_nonborn, 'DMDYRSUS'] = (age[citizen_nonborn] - 5).clip(lower=0).apply(age_to_dmdyrsus)\n",
    "\n",
    "    # Case 3: Not a citizen → assume recent arrival\n",
    "    noncitizen_mask = mask_na & non_citizen\n",
    "    df.loc[noncitizen_mask, 'DMDYRSUS'] = (age[noncitizen_mask] - 5).clip(lower=0).apply(age_to_dmdyrsus)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec00f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8 = fix_demo_advanced(df_8)\n",
    "df_desc[8] = \"Fixed demo_df advanced issues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bce94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize_codes_by_lists(df, list_777_999=None, list_77_99=None, list_7_9=None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    list_777_999 = list_777_999 or []\n",
    "    list_77_99 = list_77_99 or []\n",
    "    list_7_9 = list_7_9 or []\n",
    "    \n",
    "    # Rule 1: Replace 999 with 777\n",
    "    for col in list_777_999:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({999: 777})\n",
    "    \n",
    "    # Rule 2: Replace 99 with 77\n",
    "    for col in list_77_99:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({99: 77})\n",
    "    \n",
    "    # Rule 3: Replace 9 with 7\n",
    "    for col in list_7_9:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({9: 7})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "35ada345",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = []\n",
    "a2 = ['DMDBORN4','DMDYRSUS','DMDEDUC3','DMDMARTL','DMDHRBR4','DMDHRMAR','INDHHIN2','INDFMIN2']\n",
    "a3 = ['DMQMILIZ','DMQADFC','DMDCITZN','DMDEDUC2','DMDHREDU','DMDHSEDU']\n",
    "\n",
    "b1 = ['RHQ010','RHQ060','RHQ163','RHD173','RHD180','RHD190','RHQ197','RHQ291','RHQ332']\n",
    "b2 = ['RHD043','RHQ070','RHQ160','RHQ166','RHQ169','RHQ171','RHQ542A','RHQ560Q','RHQ576Q','RHQ586Q','RHQ602Q']\n",
    "b3 = ['RHQ020','RHQ031','RHQ074','RHQ076','RHQ078','RHQ131','RHD143','RHQ162','RHQ172','RHQ200','RHD280','RHQ305','RHQ420','RHQ540','RHQ554','RHQ560U','RHQ570','RHQ576U','RHQ580','RHQ586U','RHQ596','RHQ602U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "837bd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_9 = df_7.copy()\n",
    "df_10 = df_8.copy()\n",
    "\n",
    "df_9 = normalize_codes_by_lists(\n",
    "    df_9,\n",
    "    list_777_999=b1,\n",
    "    list_77_99=b2,\n",
    "    list_7_9=b3\n",
    ")\n",
    "\n",
    "df_10 = normalize_codes_by_lists(\n",
    "    df_10,\n",
    "    list_777_999=a1,\n",
    "    list_77_99=a2,\n",
    "    list_7_9=a3\n",
    ")\n",
    "\n",
    "df_desc[9] = \"Normalized codes in rhq\"\n",
    "df_desc[10] = \"Normalized codes in demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c340ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5815, 115)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_11 = df_10.copy()\n",
    "df_11 = df_11.merge(pbcd_df, on='SEQN', how='right')\n",
    "df_11 = df_11.merge(tst_df, on='SEQN', how='left')\n",
    "df_11 = df_11.merge(df_9, on='SEQN', how='left')\n",
    "df_11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7cb80fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11[\"AgeGroup_Years\"] = pd.cut(\n",
    "    df_11[\"RIDAGEYR\"],\n",
    "    bins=[-1, 12, 19, 29, 39, 49, 59, float(\"inf\")],\n",
    "    labels=[\n",
    "        \"0-12 (Child)\",\n",
    "        \"13-19 (Teen)\",\n",
    "        \"20-29 (Young Adult)\",\n",
    "        \"30-39\",\n",
    "        \"40-49\",\n",
    "        \"50-59\",\n",
    "        \"60+\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Bucket by months (same cutoffs but in months)\n",
    "df_11[\"AgeGroup_Months\"] = pd.cut(\n",
    "    df_11[\"RIDAGEMN\"],\n",
    "    bins=[-1, 239, 540, float(\"inf\")],\n",
    "    labels=[\"Under 20y\", \"20-45y\", \"Above 45y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4012b4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4059"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_11['LBXEST'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4a9bc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11 = mark_male_nans(df_11,male_code=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6c1b6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11_2 = df_11.copy()\n",
    "df_11_2 = df_11.merge(bmx_df, on='SEQN', how='left')\n",
    "df_desc[11.2] = \"all merged for demo, tst, pbcd, rhq, bmx order\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fc5f9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11_3 = df_11_2.merge(ihgem_df, on='SEQN', how='left')\n",
    "df_desc[11.3] = \"all merged for demo, tst, pbcd, rhq, bmx, umi order\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a021e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11_2.loc[(df_11_2[\"RIDAGEMN\"] < 12 * 20) & (df_11_2[\"RHQ131\"].isna()), \"RHQ131\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4352fe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier counts above thresholds:\n",
      "         LBXEST  LBXTST  LBXSHBG\n",
      "0.1        4059    4090     3937\n",
      "0.2        4059    4090     3937\n",
      "0.3        4059    4090     3937\n",
      "0.5        4059    4090     3937\n",
      "1.0        4059    4046     3937\n",
      "50.0        651    1629     2095\n",
      "100.0       370    1539      719\n",
      "200.0       129    1414       53\n",
      "300.0        59    1120       24\n",
      "400.0        34     712       11\n",
      "500.0        20     420        8\n",
      "600.0        15     218        3\n",
      "700.0        15     106        2\n",
      "800.0        14      53        1\n",
      "1000.0       12      12        0\n",
      "1200.0       12       5        0\n",
      "1400.0       11       2        0\n",
      "1600.0       10       2        0\n",
      "2000.0       10       0        0\n",
      "3000.0        6       0        0\n",
      "4000.0        5       0        0\n",
      "6000.0        4       0        0\n",
      "7000.0        4       0        0\n",
      "8000.0        3       0        0\n",
      "9000.0        3       0        0\n",
      "11000.0       1       0        0\n",
      "12000.0       1       0        0\n",
      "13000.0       0       0        0\n",
      "14000.0       0       0        0\n",
      "15000.0       0       0        0\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.1, 0.2, 0.3, 0.5 ,1 ,50,100,200, 300, 400, 500,600,700, 800, 1000,1200, 1400, 1600, 2000, 3000, 4000, 6000, 7000, 8000, 9000, 11000, 12000, 13000, 14000, 15000]\n",
    "\n",
    "# count how many values are above each threshold for each column\n",
    "outlier_counts = {}\n",
    "for col in ['LBXEST', 'LBXTST','LBXSHBG']:\n",
    "    outlier_counts[col] = {thr: (df_11_2[col] > thr).sum() for thr in thresholds}\n",
    "\n",
    "outlier_counts_df = pd.DataFrame(outlier_counts)\n",
    "print(\"Outlier counts above thresholds:\")\n",
    "print(outlier_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c4948db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier counts above thresholds:\n",
      "        LBDBPBSI  LBDBCDSI  LBDTHGSI  LBDBSESI  LBDBMNSI\n",
      "0.1          499      4988      4988      4987      4987\n",
      "0.2           83      4988      4988      4987      4987\n",
      "0.3           30      4988      4988      4987      4987\n",
      "0.5            6      4988      4988      4987      4987\n",
      "1.0            1      3300      3718      4986      4987\n",
      "2.0            0      1862      2799      4466      4987\n",
      "4.0            0       821      1561         7      4987\n",
      "6.0            0       480      1018         0      4987\n",
      "10.0           0       220       576         0      4987\n",
      "20.0           0        48       237         0      4987\n",
      "50.0           0         2        46         0      4984\n",
      "75.0           0         1        16         0      4959\n",
      "100.0          0         0         7         0      4810\n",
      "150.0          0         0         2         0      3613\n",
      "200.0          0         0         0         0      1918\n",
      "300.0          0         0         0         0       344\n",
      "400.0          0         0         0         0        59\n",
      "500.0          0         0         0         0        12\n",
      "520.0          0         0         0         0        10\n",
      "540.0          0         0         0         0         8\n",
      "575.0          0         0         0         0         6\n",
      "600.0          0         0         0         0         4\n",
      "625.0          0         0         0         0         4\n",
      "650.0          0         0         0         0         4\n",
      "700.0          0         0         0         0         3\n",
      "800.0          0         0         0         0         3\n",
      "1000.0         0         0         0         0         2\n",
      "1300.0         0         0         0         0         1\n",
      "2000.0         0         0         0         0         0\n",
      "3000.0         0         0         0         0         0\n",
      "4000.0         0         0         0         0         0\n",
      "6000.0         0         0         0         0         0\n",
      "7000.0         0         0         0         0         0\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.1, 0.2, 0.3, 0.5 ,1,2,4,6,10,20,50,75,100,150,200, 300, 400, 500,520,540,575,600,625,650,700, 800, 1000,1300, 2000, 3000, 4000, 6000, 7000, ]\n",
    "metal_cols = ['LBDBPBSI', 'LBDBCDSI', 'LBDTHGSI', 'LBDBSESI', 'LBDBMNSI']\n",
    "\n",
    "# count how many values are above each threshold for each column\n",
    "outlier_counts = {}\n",
    "for col in metal_cols:\n",
    "    outlier_counts[col] = {thr: (df_11_2[col] > thr).sum() for thr in thresholds}\n",
    "\n",
    "outlier_counts_df = pd.DataFrame(outlier_counts)\n",
    "print(\"Outlier counts above thresholds:\")\n",
    "print(outlier_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "567f6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_11_2  = df_11_2[df_11_2[\"LBDBMNSI\"]  <= 1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d2c66204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = df_11_2.dropna(subset=[\"LBXEST\"])\n",
    "df_13 = df_13.reset_index(drop=True)\n",
    "df_desc[13] = \"target column - LBXEST not null\"\n",
    "\n",
    "df_14 = df_11_2.dropna(subset=[\"LBXTST\"])\n",
    "df_14 = df_14.reset_index(drop=True)\n",
    "df_desc[14] = \"target column - LBXTST not null\"\n",
    "\n",
    "df_15 = df_11_2.dropna(subset=[\"LBXTST\"])\n",
    "df_15 = df_15.reset_index(drop=True)\n",
    "df_desc[15] = \"target column - LBXSHBG not null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698c796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c15ee787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_16  = df_13[df_13[\"LBXEST\"]  <= 2500].copy()\n",
    "df_desc[16] = \"df_13 with LBXEST with <= 1200 outliers removed\"\n",
    "\n",
    "df_17  = df_14[df_14[\"LBXTST\"]  <= 2200].copy()\n",
    "df_desc[17] = \"df_14 with LBXTST with <= 1200 outliers removed\"\n",
    "\n",
    "df_18  = df_15[df_15[\"LBXSHBG\"]  <= 500].copy()\n",
    "df_desc[18] = \"df_15 with LBXSHBG with <= 500 outliers removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a0267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d00985b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4059, 152)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns = [\n",
    "    #'LBXTST',      \n",
    "    'LBXEST',      # Estradiol (regression)\n",
    "    #'LBXSHBG',     # SHBG (regression)\n",
    "    #'LBDTSTLC',    # Testosterone Status (classifi cation)\n",
    "    #'LBDESTLC',    # Estradiol Status (classification)\n",
    "    #'LBDSHGLC',    # SHBG Status (classification)\n",
    "]\n",
    "\n",
    "# Drop rows that have missing values in ANY of the target columns\n",
    "df_12 = df_11_3.dropna(subset=target_columns)\n",
    "\n",
    "# Optional: Reset index after dropping\n",
    "df_12 = df_12.reset_index(drop=True)\n",
    "df_desc[12] = \"df_11 with target - LBXEST\"\n",
    "df_12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7c198b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '20-29 (Young Adult)'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_21968\\3926739195.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_feature_importance(\n\u001b[32m      2\u001b[39m     df=df_12,\n\u001b[32m      3\u001b[39m     target_col=\u001b[33m'LBXEST'\u001b[39m,\n\u001b[32m      4\u001b[39m     drop_cols=[\u001b[33m'LBXTST'\u001b[39m, \u001b[33m'LBDTSTLC'\u001b[39m, \u001b[33m'LBDESTLC'\u001b[39m, \u001b[33m'LBXSHBG'\u001b[39m, \u001b[33m'LBDSHGLC'\u001b[39m, \u001b[33m'SEQN'\u001b[39m, \u001b[33m'LBXEST'\u001b[39m, \u001b[33m'RHD143'\u001b[39m, \u001b[33m'RIDEXPRG'\u001b[39m, \u001b[33m'RHD180'\u001b[39m, \u001b[33m'RHQ169'\u001b[39m,\u001b[33m'RHQ197'\u001b[39m,\u001b[33m'DMDHRAGE'\u001b[39m],\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_21968\\3326451468.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, target_col, drop_cols, model_type, test_size, random_state, top_n)\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"model_type must be 'regression' or 'classification'\"\u001b[39m)\n\u001b[32m    229\u001b[39m \n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     model.fit(X_train, y_train)\n\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Feature importances\u001b[39;00m\n\u001b[32m    234\u001b[39m     importances = model.feature_importances_\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    359\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    360\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"sparse multilabel-indicator for y is not supported.\"\u001b[39m)\n\u001b[32m    361\u001b[39m \n\u001b[32m    362\u001b[39m         X, y = validate_data(\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m             self,\n\u001b[32m    364\u001b[39m             X,\n\u001b[32m    365\u001b[39m             y,\n\u001b[32m    366\u001b[39m             multi_output=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n",
      "\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2149\u001b[39m     def __array__(\n\u001b[32m   2150\u001b[39m         self, dtype: npt.DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2151\u001b[39m     ) -> np.ndarray:\n\u001b[32m   2152\u001b[39m         values = self._values\n\u001b[32m-> \u001b[39m\u001b[32m2153\u001b[39m         arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2154\u001b[39m         if (\n\u001b[32m   2155\u001b[39m             astype_is_view(values.dtype, arr.dtype)\n\u001b[32m   2156\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m using_copy_on_write()\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '20-29 (Young Adult)'"
     ]
    }
   ],
   "source": [
    "plot_feature_importance(\n",
    "    df=df_12,\n",
    "    target_col='LBXEST',\n",
    "    drop_cols=['LBXTST', 'LBDTSTLC', 'LBDESTLC', 'LBXSHBG', 'LBDSHGLC', 'SEQN', 'LBXEST', 'RHD143', 'RIDEXPRG', 'RHD180', 'RHQ169','RHQ197','DMDHRAGE'],\n",
    "    model_type='regression',\n",
    "    top_n=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1142d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "def preprocess_and_model(df, target_col, drop_cols=None, test_size=0.2, random_state=42, top_features=60):\n",
    "    \"\"\"\n",
    "    df: input dataframe with target column included\n",
    "    target_col: name of the target column as string\n",
    "    drop_cols: list of column names to drop before modeling\n",
    "    test_size: fraction for test split\n",
    "    random_state: for reproducibility\n",
    "    top_features: number of features to select with RandomForest\n",
    "\n",
    "    Returns dictionary with performance metrics and best params of models\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop ignored columns\n",
    "    if drop_cols:\n",
    "        drop_existing = [c for c in drop_cols if c in df.columns]\n",
    "        df.drop(columns=drop_existing, inplace=True)\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Identify columns by data type and unique values\n",
    "    yes_no_cols = [col for col in X.columns if set(X[col].dropna().unique()).issubset({'yes', 'no', 'Yes', 'No', 'YES', 'NO'})]\n",
    "    cat_cols_oh = [col for col in X.columns if X[col].nunique() < 5 and col not in yes_no_cols and X[col].dtype == 'object']\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    high_card_cols = [col for col in numeric_cols if X[col].nunique() > 30]\n",
    "    low_card_num_cols = [col for col in numeric_cols if X[col].nunique() <= 30]\n",
    "\n",
    "    # Imputers\n",
    "    yes_no_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Process Yes/No columns\n",
    "    if yes_no_cols:\n",
    "        X_yes_no_imp = yes_no_imputer.fit_transform(X[yes_no_cols])\n",
    "        X_yes_no = pd.DataFrame(X_yes_no_imp, columns=yes_no_cols)\n",
    "        for col in yes_no_cols:\n",
    "            X_yes_no[col] = X_yes_no[col].str.lower().map({'yes': 1, 'no': 0})\n",
    "    else:\n",
    "        X_yes_no = pd.DataFrame()\n",
    "\n",
    "    # Process categorical columns (<5 unique, excluding yes/no)\n",
    "    if cat_cols_oh:\n",
    "        X_cat_imp = cat_imputer.fit_transform(X[cat_cols_oh])\n",
    "        X_cat = pd.DataFrame(X_cat_imp, columns=cat_cols_oh)\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_cat_ohe = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names_out(cat_cols_oh))\n",
    "    else:\n",
    "        X_cat_ohe = pd.DataFrame()\n",
    "\n",
    "    # High-cardinality numeric (scale)\n",
    "    if high_card_cols:\n",
    "        X_high_card_imp = num_imputer.fit_transform(X[high_card_cols])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_high_card = pd.DataFrame(scaler.fit_transform(X_high_card_imp), columns=high_card_cols)\n",
    "    else:\n",
    "        X_high_card = pd.DataFrame()\n",
    "\n",
    "    # Low-cardinality numeric (no scale)\n",
    "    if low_card_num_cols:\n",
    "        X_low_card_imp = num_imputer.fit_transform(X[low_card_num_cols])\n",
    "        X_low_card_num = pd.DataFrame(X_low_card_imp, columns=low_card_num_cols)\n",
    "    else:\n",
    "        X_low_card_num = pd.DataFrame()\n",
    "\n",
    "    # Combine processed data\n",
    "    X_processed = pd.concat([X_yes_no, X_cat_ohe, X_high_card, X_low_card_num], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Feature selection using RandomForest\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(top_features).index.tolist()\n",
    "\n",
    "    X_train_sel = X_train[top_feats]\n",
    "    X_test_sel = X_test[top_feats]\n",
    "\n",
    "    # Models\n",
    "    xgb = XGBRegressor(random_state=random_state, objective='reg:squarederror', eval_metric='mae')\n",
    "    huber = HuberRegressor()\n",
    "\n",
    "    xgb_param_grid = {'n_estimators': [50,100, 500, 1000], 'max_depth': [3, 5, 7, 12], 'learning_rate': [0.05, 0.1, 0.001, 0.01]}\n",
    "    huber_param_grid = {'epsilon': [1.1, 1.35, 1.5], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "    xgb_gs = GridSearchCV(xgb, xgb_param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    huber_gs = GridSearchCV(huber, huber_param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "    xgb_gs.fit(X_train_sel, y_train)\n",
    "    huber_gs.fit(X_train_sel, y_train)\n",
    "\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    results = {\n",
    "        'XGBoost Regressor': {'Best Params': xgb_gs.best_params_, 'Performance': evaluate_model(xgb_gs.best_estimator_, X_test_sel, y_test)},\n",
    "        'Huber Regressor': {'Best Params': huber_gs.best_params_, 'Performance': evaluate_model(huber_gs.best_estimator_, X_test_sel, y_test)},\n",
    "        'Selected Features': [f'{feature} : {columns_names[feature]}' for feature in top_feats],\n",
    "        #'Selected Features': top_feats,\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf285ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'XGBoost Regressor': {'Best Params': {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100},\n",
       "  'Performance': {'MAE': 42.16568493535131,\n",
       "   'MSE': 98012.21958941719,\n",
       "   'RMSE': 313.06903326489703,\n",
       "   'R2': 0.09478936114430303}},\n",
       " 'Huber Regressor': {'Best Params': {'alpha': 0.01, 'epsilon': 1.1},\n",
       "  'Performance': {'MAE': 41.364078436929326,\n",
       "   'MSE': 106851.4582657653,\n",
       "   'RMSE': 326.88141315432006,\n",
       "   'R2': 0.013152878237032928}},\n",
       " 'Selected Features': ['RHD143 : Are you pregnant now?',\n",
       "  'LBDTHGSI : Blood mercury, total (nmol/L)',\n",
       "  'WTSH2YR : Blood metal weights',\n",
       "  'LBDBSESI : Blood selenium (umol/L)',\n",
       "  'LBDBCDSI : Blood cadmium (umol/L)',\n",
       "  'RIDAGEMN : Age in months at screening - 0 to 24 mos',\n",
       "  'LBDBPBSI : Blood lead (umol/L)',\n",
       "  'RHQ160 : How many times have been pregnant?',\n",
       "  'RIDEXPRG : Pregnancy status at exam',\n",
       "  'LBDBMNSI : Blood manganese (umol/L)',\n",
       "  'RHQ078 : Ever treated for a pelvic infection/PID?',\n",
       "  'RHQ131 : Ever been pregnant?',\n",
       "  'RIAGENDR : Gender']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns you want to include (plus target)\n",
    "cols_to_use = [\n",
    "    'LBXEST',\n",
    "    #'LBXBSE',\n",
    "  'LBDBSESI',\n",
    "  'RIDEXPRG',\n",
    "  #'RHD180',\n",
    "  #'RHQ169',\n",
    "  'LBDTHGSI',\n",
    "  'RHQ160',\n",
    "  #'LBXTHG',\n",
    "  'LBDBCDSI',\n",
    "  'LBDBPBSI',\n",
    "  'LBDBMNSI',\n",
    "  'RIDAGEMN',\n",
    "  #'RIDAGEYR',\n",
    "  #'LBXBCD',\n",
    "  #'INDFMPIR',\n",
    "  #'RHQ542D',\n",
    "  #'INDHHIN2',\n",
    "  #'WTINT2YR',\n",
    "  #'WTMEC2YR',\n",
    "  'WTSH2YR',\n",
    "  #'RHD190',\n",
    "  #'LBXBPB',\n",
    "  #'LBXBPB',\n",
    "  'RIAGENDR',\n",
    "  'RHQ078',\n",
    "  'RHQ131',\n",
    "   'RHD143',\n",
    "   ]\n",
    "\n",
    "# Filter df to keep only these columns\n",
    "df_filtered = df_12[cols_to_use]\n",
    "\n",
    "# Call your function with df_filtered\n",
    "preprocess_and_model(\n",
    "    df_filtered,\n",
    "    target_col='LBXEST',\n",
    "    top_features=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1b352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blood selenium (ug/L)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_names['LBXBSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_menopause_features(df):\n",
    "    # Initialize columns\n",
    "    df[\"is_menopausal\"] = np.nan\n",
    "    df[\"menopause_age\"] = np.nan\n",
    "\n",
    "    # Exclude men (-1 means male record per your note)\n",
    "    df.loc[df[\"RHQ031\"] == -1, [\"is_menopausal\", \"menopause_age\"]] = np.nan\n",
    "\n",
    "    # Surgical menopause: hysterectomy or ovaries removed\n",
    "    surgical_mask = (df[\"RHD280\"] == 1) | (df[\"RHQ305\"] == 1)\n",
    "    df.loc[surgical_mask, \"is_menopausal\"] = 1\n",
    "    \n",
    "    # Determine menopause age for surgical cases\n",
    "    df.loc[surgical_mask, \"menopause_age\"] = df[[\"RHQ291\", \"RHQ332\"]].min(axis=1)\n",
    "\n",
    "    # Natural menopause: no periods in last 12 months\n",
    "    natural_mask = (df[\"RHQ031\"] == 2)\n",
    "    df.loc[natural_mask, \"is_menopausal\"] = 1\n",
    "    df.loc[natural_mask, \"menopause_age\"] = df[\"RHQ060\"]\n",
    "\n",
    "    # Still menstruating\n",
    "    still_mask = (df[\"RHQ031\"] == 1)\n",
    "    df.loc[still_mask, \"is_menopausal\"] = 0\n",
    "    df.loc[still_mask, \"menopause_age\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "df_19 = add_menopause_features(df_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f1c54e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_menopause_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Initialize columns\n",
    "    df[\"is_menopausal\"] = np.nan\n",
    "    df[\"menopause_age\"] = np.nan\n",
    "\n",
    "    # Surgical menopause: hysterectomy or ovaries removed\n",
    "    surgical_mask = (df[\"RHD280\"] == 1) | (df[\"RHQ305\"] == 1)\n",
    "    df.loc[surgical_mask, \"is_menopausal\"] = 1\n",
    "    df.loc[surgical_mask, \"menopause_age\"] = df[[\"RHQ291\", \"RHQ332\"]].min(axis=1)\n",
    "\n",
    "    # Natural menopause: no periods in last 12 months\n",
    "    natural_mask = (df[\"RHQ031\"] == 2)\n",
    "    df.loc[natural_mask, \"is_menopausal\"] = 1\n",
    "    df.loc[natural_mask, \"menopause_age\"] = df[\"RHQ060\"]\n",
    "\n",
    "    # Still menstruating\n",
    "    still_mask = (df[\"RHQ031\"] == 1)\n",
    "    df.loc[still_mask, \"is_menopausal\"] = 0\n",
    "    df.loc[still_mask, \"menopause_age\"] = np.nan\n",
    "\n",
    "    # ✅ Override based on gender\n",
    "    df.loc[df[\"RIAGENDR\"] == 1, \"is_menopausal\"] = -1   # males\n",
    "    df.loc[df[\"RIAGENDR\"] == 2, \"is_menopausal\"] = df[\"is_menopausal\"].fillna(0)  # females: ensure 0 if missing\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df_19 = add_menopause_features(df_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af64dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19 = mark_male_nans(df_19,male_code=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ec99e13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "  self.scorer_ = scorers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model saved to ./models/LBXEST/xgboost_model_0.joblib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Selected Features</th>\n",
       "      <th>Target Variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 20}</td>\n",
       "      <td>27.719546</td>\n",
       "      <td>3475.655786</td>\n",
       "      <td>58.954693</td>\n",
       "      <td>0.471077</td>\n",
       "      <td>[RIDEXPRG, BMXBMI, LBDBMNSI, LBDBCDSI, LBDTHGSI, RIDAGEMN, LBDBSESI, LBDBPBSI, RHQ031, RHQ131, is_menopausal, RIAGENDR]</td>\n",
       "      <td>LBXEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                                  Best Params  \\\n",
       "0  XGBRegressor  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 20}   \n",
       "\n",
       "         MAE          MSE       RMSE        R2  \\\n",
       "0  27.719546  3475.655786  58.954693  0.471077   \n",
       "\n",
       "                                                                                                         Selected Features  \\\n",
       "0  [RIDEXPRG, BMXBMI, LBDBMNSI, LBDBCDSI, LBDTHGSI, RIDAGEMN, LBDBSESI, LBDBPBSI, RHQ031, RHQ131, is_menopausal, RIAGENDR]   \n",
       "\n",
       "  Target Variable  \n",
       "0          LBXEST  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns you want to include (plus target)\n",
    "cols_to_use = [\n",
    "    #'LBXEST',\n",
    "    #'LBXBSE',|\n",
    "    'LBXEST',\n",
    "    #'LBXSHBG',\n",
    "  'LBDBSESI',\n",
    "  #'RHD180',\n",
    "  #'RHQ169',\n",
    "  'LBDTHGSI',\n",
    "  #'LBXTHG',\n",
    "  'LBDBCDSI',\n",
    "  'LBDBPBSI',\n",
    "  'RIDAGEMN',\n",
    "  # 'RHQ160',\n",
    "  'LBDBMNSI',\n",
    "  #'RIDAGEYR',\n",
    "  #'LBXBCD',\n",
    "  #'INDFMPIR',\n",
    "  #'RHQ542D',\n",
    "  #'INDHHIN2',\n",
    "  #'WTINT2YR',\n",
    "  #'WTMEC2YR',\n",
    "  #'WTSH2YR',\n",
    "  #'RHD190',\n",
    "  #'LBXBPB',\n",
    "  #'LBXBPB',\n",
    "  #'INDFMPIR',\n",
    "  'RHQ131',\n",
    "  'RIAGENDR',\n",
    "  'RIDEXPRG',\n",
    "  #'BMXWT',\n",
    "  'BMXBMI',\n",
    "  # \"BMDSADCM\",\n",
    "  #\"AgeGroup_Years\",\n",
    "#   \"RHQ420\",\n",
    "#   \"RHQ540\",\n",
    "# \"RHQ200\",\n",
    "#   \"RHD280\",\n",
    "#   \"RHQ074\",\n",
    "#\"RHQ305\",\n",
    "   \"RHQ031\",\n",
    "  \"is_menopausal\",\n",
    "]\n",
    "\n",
    "\n",
    "# Filter df to keep only these columns\n",
    "df_filtered_est = df_19[cols_to_use]\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "results, results_dic = preprocess_and_model_shap_save_2(\n",
    "    df_filtered_est,\n",
    "    target_col=\"LBXEST\",\n",
    "    model=XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "    param_grid={\n",
    "        \"n_estimators\": [20, 100, 500, 1000],\n",
    "        \"max_depth\": [3, 5, 7, 12],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.001, 0.01]\n",
    "    },\n",
    "    drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "    top_features=20,\n",
    "    save_model=True,\n",
    "    model_path=\"./models/LBXEST/xgboost_model_0.joblib\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d65165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4050 entries, 0 to 4058\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   LBXEST         4050 non-null   float64\n",
      " 1   LBDBSESI       4047 non-null   float64\n",
      " 2   LBDTHGSI       4048 non-null   float64\n",
      " 3   LBDBCDSI       4048 non-null   float64\n",
      " 4   LBDBPBSI       4048 non-null   float64\n",
      " 5   RIDAGEMN       4050 non-null   float64\n",
      " 6   LBDBMNSI       4047 non-null   float64\n",
      " 7   RHQ131         3927 non-null   float64\n",
      " 8   RIDEXPRG       4050 non-null   float64\n",
      " 9   BMXBMI         4018 non-null   float64\n",
      " 10  RHQ031         3436 non-null   float64\n",
      " 11  is_menopausal  4050 non-null   float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 540.4 KB\n"
     ]
    }
   ],
   "source": [
    "df_filtered_est.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ace05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RHQ131\n",
       "-1.0      2024\n",
       " 1.0       946\n",
       " 3.0       737\n",
       " 2.0       206\n",
       " 500.0      13\n",
       " 7.0         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered_est['RHQ131'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bdae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered_est[df_filtered_est['RIDAGEMN'] < 12 * 20]['RHQ131'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4702c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training XGBoost...\n",
      "✅ Best model saved to ./models/LBXEST/xgboost_model.joblib\n",
      "\n",
      "🔹 Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1350\n",
      "[LightGBM] [Info] Number of data points in the train set: 3235, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 34.644773\n",
      "✅ Best model saved to ./models/LBXEST/lightgbm_model.joblib\n",
      "\n",
      "🔹 Training RandomForest...\n",
      "✅ Best model saved to ./models/LBXEST/randomforest_model.joblib\n",
      "\n",
      "🔹 Training GradientBoosting...\n",
      "✅ Best model saved to ./models/LBXEST/gradientboosting_model.joblib\n",
      "\n",
      "🔹 Training SVR...\n",
      "✅ Best model saved to ./models/LBXEST/svr_model.joblib\n",
      "\n",
      "🔹 Training LogisticRegression...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 18 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1231, in fit\n    check_classification_targets(y)\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 219, in check_classification_targets\n    raise ValueError(\nValueError: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, (model, param_grid) \u001b[38;5;129;01min\u001b[39;00m models_and_params.items():\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔹 Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     results, results_dic = preprocess_and_model_shap_save(\n\u001b[32m     67\u001b[39m         df_filtered_est,\n\u001b[32m     68\u001b[39m         target_col=\u001b[33m\"\u001b[39m\u001b[33mLBXEST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     69\u001b[39m         model=model,\n\u001b[32m     70\u001b[39m         param_grid=param_grid,\n\u001b[32m     71\u001b[39m         drop_cols=[\u001b[33m\"\u001b[39m\u001b[33mSEQN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDMDHRAGE\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     72\u001b[39m         top_features=\u001b[32m20\u001b[39m,\n\u001b[32m     73\u001b[39m         save_model=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     74\u001b[39m         model_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./models/LBXEST/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model.joblib\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m         run_shap=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     77\u001b[39m     results_all[model_name] = results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 933\u001b[39m, in \u001b[36mpreprocess_and_model_shap_save\u001b[39m\u001b[34m(df, target_col, model, param_grid, drop_cols, test_size, random_state, top_features, run_shap, external_results_path, save_model, model_path)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# ---------------- Model training with GridSearch ----------------\u001b[39;00m\n\u001b[32m    931\u001b[39m gs = GridSearchCV(model, param_grid, cv=\u001b[32m3\u001b[39m,\n\u001b[32m    932\u001b[39m                   scoring=\u001b[33m'\u001b[39m\u001b[33mneg_mean_absolute_error\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m gs.fit(X_train_sel, y_train)\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# ---------------- Evaluation ----------------\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mevaluate_model\u001b[39m(model, X_test, y_test):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1012\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1013\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1014\u001b[39m     )\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m \u001b[38;5;28mself\u001b[39m._run_search(evaluate_candidates)\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1022\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1571\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1572\u001b[39m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m.param_grid))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:995\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m    993\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m.error_score)\n\u001b[32m    997\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    523\u001b[39m     all_fits_failed_message = (\n\u001b[32m    524\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    525\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    532\u001b[39m     some_fits_failed_message = (\n\u001b[32m    533\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    534\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    538\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    539\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 18 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1231, in fit\n    check_classification_targets(y)\n  File \"d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 219, in check_classification_targets\n    raise ValueError(\nValueError: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define models and parameter grids\n",
    "models_and_params = {\n",
    "    \"XGBoost\": (\n",
    "        XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "        {\n",
    "            \"n_estimators\": [100, 500],\n",
    "            \"max_depth\": [3, 6, 10],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 500],\n",
    "            \"max_depth\": [-1, 10],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"num_leaves\": [31, 64]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [100, 300],\n",
    "            \"max_depth\": [5, 10, None]\n",
    "        }\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 300],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"SVR\": (\n",
    "        SVR(),\n",
    "        {\n",
    "            \"C\": [0.1, 1,],\n",
    "            \"kernel\": [\"rbf\", \"linear\"]\n",
    "        }\n",
    "    ),\n",
    "    \"LinearRegression\": (\n",
    "        LinearRegression(),\n",
    "        {}  # no hyperparameters to tune\n",
    "    ),\n",
    "}\n",
    "\n",
    "results_all = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, (model, param_grid) in models_and_params.items():\n",
    "    print(f\"\\n🔹 Training {model_name}...\")\n",
    "    results, results_dic = preprocess_and_model_shap_save(\n",
    "        df_filtered_est,\n",
    "        target_col=\"LBXEST\",\n",
    "        model=model,\n",
    "        param_grid=param_grid,\n",
    "        drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "        top_features=20,\n",
    "        save_model=True,\n",
    "        model_path=f\"./models/LBXEST/{model_name.lower()}_model.joblib\",\n",
    "        run_shap=False\n",
    "    )\n",
    "    results_all[model_name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "  self.scorer_ = scorers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model saved to ./models/LBXTST/xgboost_model_0.joblib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Selected Features</th>\n",
       "      <th>Target Variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 20}</td>\n",
       "      <td>92.213559</td>\n",
       "      <td>18405.711443</td>\n",
       "      <td>135.667651</td>\n",
       "      <td>0.653123</td>\n",
       "      <td>[RIDAGEMN, RIAGENDR, RIDEXPRG, BMXBMI, LBDBMNSI, LBDBSESI, LBDBCDSI, LBDBPBSI, LBDTHGSI, RHQ131]</td>\n",
       "      <td>LBXTST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                                  Best Params  \\\n",
       "0  XGBRegressor  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 20}   \n",
       "\n",
       "         MAE           MSE        RMSE        R2  \\\n",
       "0  92.213559  18405.711443  135.667651  0.653123   \n",
       "\n",
       "                                                                                  Selected Features  \\\n",
       "0  [RIDAGEMN, RIAGENDR, RIDEXPRG, BMXBMI, LBDBMNSI, LBDBSESI, LBDBCDSI, LBDBPBSI, LBDTHGSI, RHQ131]   \n",
       "\n",
       "  Target Variable  \n",
       "0          LBXTST  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns you want to include (plus target)\n",
    "cols_to_use = [\n",
    "    'LBXTST',\n",
    "    #'LBXBSE',\n",
    "    #'LBXEST',\n",
    "    #'LBXSHBG',\n",
    "  'LBDBSESI',\n",
    "  #'RHD180',\n",
    "  #'RHQ169',\n",
    "  'LBDTHGSI',\n",
    "  #'LBXTHG',\n",
    "  'LBDBCDSI',\n",
    "  'LBDBPBSI',\n",
    "  'RIDAGEMN',\n",
    "#   'RHQ160',\n",
    "  'LBDBMNSI',\n",
    "  #'RIDAGEYR',\n",
    "  #'LBXBCD',\n",
    "  #'INDFMPIR',\n",
    "  #'RHQ542D',\n",
    "  #'INDHHIN2',\n",
    "  #'WTINT2YR',\n",
    "  #'WTMEC2YR',\n",
    "  #'WTSH2YR',\n",
    "  #'RHD190',\n",
    "  #'LBXBPB',\n",
    "  #'LBXBPB',\n",
    "  #'INDFMPIR',\n",
    "  'RHQ131',\n",
    "  'RIAGENDR',\n",
    "  'RIDEXPRG',\n",
    "  #'BMXWT',\n",
    "  'BMXBMI',\n",
    "#   'BMDSADCM',\n",
    "  #\"AgeGroup_Years\",\n",
    "  #\"DMDMARTL\"\n",
    "]\n",
    "\n",
    "\n",
    "# Filter df to keep only these columns\n",
    "df_filtered_tst = df_17[cols_to_use]\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "results, results_dic = preprocess_and_model_shap_save_2(\n",
    "    df_filtered_tst,\n",
    "    target_col=\"LBXTST\",\n",
    "    model=XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "    param_grid={\n",
    "        \"n_estimators\": [20, 100, 500, 1000],\n",
    "        \"max_depth\": [3, 5, 7, 12],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.001, 0.01]\n",
    "    },\n",
    "    drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "    top_features=20,\n",
    "    save_model=True,\n",
    "    model_path=\"./models/LBXTST/xgboost_model_0.joblib\"\n",
    "\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b24f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training XGBoost...\n",
      "✅ Best model saved to ./models/LBXTST/xgboost_model.joblib\n",
      "\n",
      "🔹 Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1325\n",
      "[LightGBM] [Info] Number of data points in the train set: 3268, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 164.504018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "✅ Best model saved to ./models/LBXTST/lightgbm_model.joblib\n",
      "\n",
      "🔹 Training RandomForest...\n",
      "✅ Best model saved to ./models/LBXTST/randomforest_model.joblib\n",
      "\n",
      "🔹 Training GradientBoosting...\n",
      "✅ Best model saved to ./models/LBXTST/gradientboosting_model.joblib\n",
      "\n",
      "🔹 Training SVR...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, (model, param_grid) \u001b[38;5;129;01min\u001b[39;00m models_and_params.items():\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔹 Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     results, results_dic = preprocess_and_model_shap_save(\n\u001b[32m     60\u001b[39m         df_filtered_tst,\n\u001b[32m     61\u001b[39m         target_col=\u001b[33m\"\u001b[39m\u001b[33mLBXTST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     62\u001b[39m         model=model,\n\u001b[32m     63\u001b[39m         param_grid=param_grid,\n\u001b[32m     64\u001b[39m         drop_cols=[\u001b[33m\"\u001b[39m\u001b[33mSEQN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDMDHRAGE\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     65\u001b[39m         top_features=\u001b[32m20\u001b[39m,\n\u001b[32m     66\u001b[39m         save_model=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     67\u001b[39m         model_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./models/LBXTST/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model.joblib\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     68\u001b[39m         run_shap=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     69\u001b[39m \n\u001b[32m     70\u001b[39m     )\n\u001b[32m     71\u001b[39m     results_all[model_name] = results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 933\u001b[39m, in \u001b[36mpreprocess_and_model_shap_save\u001b[39m\u001b[34m(df, target_col, model, param_grid, drop_cols, test_size, random_state, top_features, run_shap, external_results_path, save_model, model_path)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# ---------------- Model training with GridSearch ----------------\u001b[39;00m\n\u001b[32m    931\u001b[39m gs = GridSearchCV(model, param_grid, cv=\u001b[32m3\u001b[39m,\n\u001b[32m    932\u001b[39m                   scoring=\u001b[33m'\u001b[39m\u001b[33mneg_mean_absolute_error\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m gs.fit(X_train_sel, y_train)\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# ---------------- Evaluation ----------------\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mevaluate_model\u001b[39m(model, X_test, y_test):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1012\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1013\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1014\u001b[39m     )\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m \u001b[38;5;28mself\u001b[39m._run_search(evaluate_candidates)\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1022\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1571\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1572\u001b[39m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m.param_grid))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    958\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    959\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    960\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    961\u001b[39m         )\n\u001b[32m    962\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m out = parallel(\n\u001b[32m    965\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    966\u001b[39m         clone(base_estimator),\n\u001b[32m    967\u001b[39m         X,\n\u001b[32m    968\u001b[39m         y,\n\u001b[32m    969\u001b[39m         train=train,\n\u001b[32m    970\u001b[39m         test=test,\n\u001b[32m    971\u001b[39m         parameters=parameters,\n\u001b[32m    972\u001b[39m         split_progress=(split_idx, n_splits),\n\u001b[32m    973\u001b[39m         candidate_progress=(cand_idx, n_candidates),\n\u001b[32m    974\u001b[39m         **fit_and_score_kwargs,\n\u001b[32m    975\u001b[39m     )\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[32m    977\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[32m    978\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(cv.split(X, y, **routed_params.splitter.split)),\n\u001b[32m    979\u001b[39m     )\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    984\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    985\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    987\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Softwares\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define models and parameter grids\n",
    "models_and_params = {\n",
    "    \"XGBoost\": (\n",
    "        XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "        {\n",
    "            \"n_estimators\": [100, 500],\n",
    "            \"max_depth\": [3, 6, 10],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 500],\n",
    "            \"max_depth\": [-1, 10],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"num_leaves\": [31, 64]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [100, 300],\n",
    "            \"max_depth\": [5, 10, None]\n",
    "        }\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 300],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"SVR\": (\n",
    "        SVR(),\n",
    "        {\n",
    "            \"C\": [10, 1],\n",
    "            \"kernel\": [\"rbf\", \"linear\"]\n",
    "        }\n",
    "    ),\n",
    "    \"LinearRegression\": (\n",
    "        LinearRegression(),\n",
    "        {}  # no hyperparameters to tune\n",
    "    ),\n",
    "}\n",
    "\n",
    "results_all = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, (model, param_grid) in models_and_params.items():\n",
    "    print(f\"\\n🔹 Training {model_name}...\")\n",
    "    results, results_dic = preprocess_and_model_shap_save(\n",
    "        df_filtered_tst,\n",
    "        target_col=\"LBXTST\",\n",
    "        model=model,\n",
    "        param_grid=param_grid,\n",
    "        drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "        top_features=20,\n",
    "        save_model=True,\n",
    "        model_path=f\"./models/LBXTST/{model_name.lower()}_model.joblib\",\n",
    "        run_shap=False\n",
    "\n",
    "    )\n",
    "    results_all[model_name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "  self.scorer_ = scorers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model saved to ./models/LBXSHBG/xgboost_model_0.joblib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Selected Features</th>\n",
       "      <th>Target Variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 20}</td>\n",
       "      <td>26.629312</td>\n",
       "      <td>1245.212625</td>\n",
       "      <td>35.28757</td>\n",
       "      <td>0.313169</td>\n",
       "      <td>[BMXBMI, RIDAGEMN, LBDBMNSI, RIDEXPRG, LBDBPBSI, LBDBSESI, LBDBCDSI, LBDTHGSI, RIAGENDR, RHQ131]</td>\n",
       "      <td>LBXSHBG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                                  Best Params  \\\n",
       "0  XGBRegressor  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 20}   \n",
       "\n",
       "         MAE          MSE      RMSE        R2  \\\n",
       "0  26.629312  1245.212625  35.28757  0.313169   \n",
       "\n",
       "                                                                                  Selected Features  \\\n",
       "0  [BMXBMI, RIDAGEMN, LBDBMNSI, RIDEXPRG, LBDBPBSI, LBDBSESI, LBDBCDSI, LBDTHGSI, RIAGENDR, RHQ131]   \n",
       "\n",
       "  Target Variable  \n",
       "0         LBXSHBG  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns you want to include (plus target)\n",
    "cols_to_use = [\n",
    "    #'LBXEST',\n",
    "    #'LBXBSE',\n",
    "    #'LBXEST',\n",
    "    'LBXSHBG',\n",
    "  'LBDBSESI',\n",
    "  #'RHD180',\n",
    "  #'RHQ169',\n",
    "  'LBDTHGSI',\n",
    "  #'LBXTHG',\n",
    "  'LBDBCDSI',\n",
    "  'LBDBPBSI',\n",
    "  'RIDAGEMN',\n",
    "#   'RHQ160',\n",
    "  'LBDBMNSI',\n",
    "  #'RIDAGEYR',\n",
    "  #'LBXBCD',\n",
    "  #'INDFMPIR',\n",
    "  #'RHQ542D',\n",
    "  #'INDHHIN2',\n",
    "  #'WTINT2YR',\n",
    "  #'WTMEC2YR',\n",
    "  #'WTSH2YR',\n",
    "  #'RHD190',\n",
    "  #'LBXBPB',\n",
    "  #'LBXBPB',\n",
    "  #'INDFMPIR',\n",
    "  'RHQ131',\n",
    "  'RIAGENDR',\n",
    "  'RIDEXPRG',\n",
    "  #'BMXWT',\n",
    "  'BMXBMI',\n",
    "#   'BMDSADCM',\n",
    "  #\"AgeGroup_Years\"\n",
    "]\n",
    "\n",
    "\n",
    "# Filter df to keep only these columns\n",
    "df_filtered_shbg = df_18[cols_to_use]\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "results, results_dic = preprocess_and_model_shap_save_2(\n",
    "    df_filtered_shbg,\n",
    "    target_col=\"LBXSHBG\",\n",
    "    model=XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "    param_grid={\n",
    "        \"n_estimators\": [20, 100, 500, 1000],\n",
    "        \"max_depth\": [3, 5, 7, 12],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.001, 0.01]\n",
    "    },\n",
    "    drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "    top_features=20,\n",
    "    save_model=True,\n",
    "    model_path=\"./models/LBXSHBG/xgboost_model_0.joblib\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8d98b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame saved to: csvs\\df_filtered_est.csv\n"
     ]
    }
   ],
   "source": [
    "save_df_to_csv(df_filtered_est, \"df_filtered_est\")\n",
    "# save_df_to_csv(df_filtered_tst, \"df_filtered_tst\")\n",
    "# save_df_to_csv(df_filtered_shbg, \"df_filtered_shbg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58136bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training XGBoost...\n",
      "✅ Best model saved to ./models/LBXSHBG/xgboost_model.joblib\n",
      "\n",
      "🔹 Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1325\n",
      "[LightGBM] [Info] Number of data points in the train set: 3143, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 66.716812\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "✅ Best model saved to ./models/LBXSHBG/lightgbm_model.joblib\n",
      "\n",
      "🔹 Training RandomForest...\n",
      "✅ Best model saved to ./models/LBXSHBG/randomforest_model.joblib\n",
      "\n",
      "🔹 Training GradientBoosting...\n",
      "✅ Best model saved to ./models/LBXSHBG/gradientboosting_model.joblib\n",
      "\n",
      "🔹 Training SVR...\n",
      "✅ Best model saved to ./models/LBXSHBG/svr_model.joblib\n",
      "\n",
      "🔹 Training LinearRegression...\n",
      "✅ Best model saved to ./models/LBXSHBG/linearregression_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define models and parameter grids\n",
    "models_and_params = {\n",
    "    \"XGBoost\": (\n",
    "        XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "        {\n",
    "            \"n_estimators\": [100, 500],\n",
    "            \"max_depth\": [3, 6, 10],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 500],\n",
    "            \"max_depth\": [-1, 10],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"num_leaves\": [31, 64]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [100, 300],\n",
    "            \"max_depth\": [5, 10, None]\n",
    "        }\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 300],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"SVR\": (\n",
    "        SVR(),\n",
    "        {\n",
    "            \"C\": [ 1, 10],\n",
    "            \"kernel\": [\"rbf\", \"linear\"]\n",
    "        }\n",
    "    ),\n",
    "    \"LinearRegression\": (\n",
    "        LinearRegression(),\n",
    "        {}  # no hyperparameters to tune\n",
    "    ),\n",
    "}\n",
    "\n",
    "results_all = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, (model, param_grid) in models_and_params.items():\n",
    "    print(f\"\\n🔹 Training {model_name}...\")\n",
    "    results, results_dic = preprocess_and_model_shap_save(\n",
    "        df_filtered_shbg,\n",
    "        target_col=\"LBXSHBG\",\n",
    "        model=model,\n",
    "        param_grid=param_grid,\n",
    "        drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "        top_features=20,\n",
    "        save_model=True,\n",
    "        model_path=f\"./models/LBXSHBG/{model_name.lower()}_model.joblib\",\n",
    "        run_shap=False\n",
    "    )\n",
    "    results_all[model_name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c1c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97666a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metals = [\"LBXBPB\",\"LBXBCD\",\"LBXTHG\",\"LBXBSE\",\"LBXBMN\",\"LBXIHG\",\"LBXBGE\",\"LBXBGM\"]\n",
    "horms = [\"LBXTST\",\"LBXEST\",\"LBXSHBG\"]\n",
    "corrs = df_12[metals+horms].corr().loc[horms, metals]\n",
    "\n",
    "# Make the figure wider and taller\n",
    "plt.figure(figsize=(12, 6))  # Width=12, Height=6 (adjust as needed)\n",
    "\n",
    "sns.heatmap(corrs, annot=True, vmin=-1, vmax=1, cmap=\"vlag\", \n",
    "            fmt='.3f',  # Format numbers to 3 decimal places\n",
    "            annot_kws={'size': 10})  # Make annotation text larger\n",
    "\n",
    "plt.title(\"Hormone vs metals correlation\")\n",
    "plt.tight_layout()  # Ensures everything fits nicely\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776b36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RIDEXPRG\n",
       "300.0    2024\n",
       "202.0     749\n",
       "203.0     729\n",
       "2.0       526\n",
       "1.0        17\n",
       "3.0        14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['RIDEXPRG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07704a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Original_LBXTST  Predicted_LBXTST\n",
      "0              18.20         40.980000\n",
      "1            1270.00        450.329987\n",
      "2              15.30         15.520000\n",
      "3              25.50         29.010000\n",
      "4             588.00        448.829987\n",
      "..               ...               ...\n",
      "813            33.90         23.850000\n",
      "814             5.61         16.030001\n",
      "815            23.00         29.340000\n",
      "816            11.20         32.369999\n",
      "817           309.00        462.920013\n",
      "\n",
      "[818 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Predictions\n",
    "y_pred = results_dic['best_model'].predict(results_dic[\"X_test\"])\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Original_LBXTST\": results_dic[\"y_test\"].values,\n",
    "    \"Predicted_LBXTST\": y_pred\n",
    "})\n",
    "\n",
    "# Optional: round for readability\n",
    "comparison_df = comparison_df.round(2)\n",
    "\n",
    "# Show side by side\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e285ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_LBXTST</th>\n",
       "      <th>Predicted_LBXTST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.20</td>\n",
       "      <td>40.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1270.00</td>\n",
       "      <td>450.329987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.30</td>\n",
       "      <td>15.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.50</td>\n",
       "      <td>29.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>588.00</td>\n",
       "      <td>448.829987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.90</td>\n",
       "      <td>23.530001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>269.00</td>\n",
       "      <td>398.820007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>386.00</td>\n",
       "      <td>369.609985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.78</td>\n",
       "      <td>15.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38.40</td>\n",
       "      <td>26.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.30</td>\n",
       "      <td>24.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>341.00</td>\n",
       "      <td>446.059998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17.80</td>\n",
       "      <td>28.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.20</td>\n",
       "      <td>23.469999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.30</td>\n",
       "      <td>28.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>420.00</td>\n",
       "      <td>409.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>464.00</td>\n",
       "      <td>390.649994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>63.70</td>\n",
       "      <td>27.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>140.00</td>\n",
       "      <td>398.359985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.23</td>\n",
       "      <td>9.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>243.00</td>\n",
       "      <td>360.950012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.31</td>\n",
       "      <td>8.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17.90</td>\n",
       "      <td>26.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>517.00</td>\n",
       "      <td>376.649994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.30</td>\n",
       "      <td>28.719999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.20</td>\n",
       "      <td>16.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.55</td>\n",
       "      <td>28.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21.00</td>\n",
       "      <td>28.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32.60</td>\n",
       "      <td>30.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>469.00</td>\n",
       "      <td>424.480011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>24.80</td>\n",
       "      <td>28.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>549.00</td>\n",
       "      <td>448.829987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>29.70</td>\n",
       "      <td>22.790001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8.43</td>\n",
       "      <td>9.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33.40</td>\n",
       "      <td>26.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>329.00</td>\n",
       "      <td>490.690002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38.30</td>\n",
       "      <td>26.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>500.00</td>\n",
       "      <td>446.059998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>27.40</td>\n",
       "      <td>13.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7.53</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>357.00</td>\n",
       "      <td>394.450012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>31.20</td>\n",
       "      <td>21.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.50</td>\n",
       "      <td>20.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.38</td>\n",
       "      <td>20.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.99</td>\n",
       "      <td>10.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>585.00</td>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>543.00</td>\n",
       "      <td>454.459991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>18.30</td>\n",
       "      <td>23.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.92</td>\n",
       "      <td>15.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>204.00</td>\n",
       "      <td>368.019989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original_LBXTST  Predicted_LBXTST\n",
       "0             18.20         40.980000\n",
       "1           1270.00        450.329987\n",
       "2             15.30         15.520000\n",
       "3             25.50         29.010000\n",
       "4            588.00        448.829987\n",
       "5             15.90         23.530001\n",
       "6            269.00        398.820007\n",
       "7            386.00        369.609985\n",
       "8              1.78         15.430000\n",
       "9             38.40         26.150000\n",
       "10            10.30         24.830000\n",
       "11           341.00        446.059998\n",
       "12            17.80         28.700001\n",
       "13            22.20         23.469999\n",
       "14             5.30         28.670000\n",
       "15           420.00        409.600006\n",
       "16           464.00        390.649994\n",
       "17            63.70         27.990000\n",
       "18           140.00        398.359985\n",
       "19             4.23          9.210000\n",
       "20           243.00        360.950012\n",
       "21             4.31          8.360000\n",
       "22            17.90         26.639999\n",
       "23           517.00        376.649994\n",
       "24            17.30         28.719999\n",
       "25            10.20         16.049999\n",
       "26             5.55         28.180000\n",
       "27            21.00         28.020000\n",
       "28            32.60         30.139999\n",
       "29           469.00        424.480011\n",
       "30            24.80         28.020000\n",
       "31           549.00        448.829987\n",
       "32            29.70         22.790001\n",
       "33             8.43          9.460000\n",
       "34            33.40         26.820000\n",
       "35           329.00        490.690002\n",
       "36            38.30         26.820000\n",
       "37           500.00        446.059998\n",
       "38            27.40         13.640000\n",
       "39             7.53         21.000000\n",
       "40           357.00        394.450012\n",
       "41            31.20         21.160000\n",
       "42            14.50         20.049999\n",
       "43             2.38         20.600000\n",
       "44             1.99         10.360000\n",
       "45           585.00        462.000000\n",
       "46           543.00        454.459991\n",
       "47            18.30         23.320000\n",
       "48             2.92         15.430000\n",
       "49           204.00        368.019989"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d02bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomForest...\n",
      "\n",
      "Running GradientBoosting...\n",
      "\n",
      "Running XGBoost...\n",
      "\n",
      "Running LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1066\n",
      "[LightGBM] [Info] Number of data points in the train set: 3247, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 49.035145\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Running CatBoost...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Define model registry: model class + parameter grid\n",
    "regressor_configs = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200,],\n",
    "            \"max_depth\": [5, 10, None]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBRegressor(random_state=42, verbosity=0),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [10, 50, 200, 500, 1000],\n",
    "            \"learning_rate\": [0.05, 0.1, 0.001],\n",
    "            \"max_depth\": [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"model\": LGBMRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [200, 500],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [-1, 10, 3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"model\": CatBoostRegressor(random_state=42, silent=True),\n",
    "        \"param_grid\": {\n",
    "            \"iterations\": [200, 500,],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"depth\": [7, 10, 3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "all_model_results = []\n",
    "\n",
    "for model_name, cfg in regressor_configs.items():\n",
    "    print(f\"\\nRunning {model_name}...\")\n",
    "    current_result, results_dict = preprocess_and_model_shap(\n",
    "        df=df_filtered,\n",
    "        target_col=\"LBXEST\",\n",
    "        model=cfg[\"model\"],\n",
    "        param_grid=cfg[\"param_grid\"],\n",
    "        drop_cols=[\"SEQN\", \"DMDHRAGE\"],\n",
    "        run_shap=False,\n",
    "        #external_results_path=\"all_results.csv\",\n",
    "    )\n",
    "    \n",
    "    all_model_results.append((model_name, current_result, results_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c134cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Selected Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50}</td>\n",
       "      <td>34.949970</td>\n",
       "      <td>31253.891986</td>\n",
       "      <td>176.787703</td>\n",
       "      <td>0.711349</td>\n",
       "      <td>[RIDEXPRG, LBDBSESI, LBDTHGSI, LBDBCDSI, LBDBPBSI, RIDAGEMN, RHQ160, LBDBMNSI, RIAGENDR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50}</td>\n",
       "      <td>34.949970</td>\n",
       "      <td>31253.891986</td>\n",
       "      <td>176.787703</td>\n",
       "      <td>0.711349</td>\n",
       "      <td>[RIDEXPRG, LBDBSESI, LBDTHGSI, LBDBCDSI, LBDBPBSI, RIDAGEMN, RHQ160, LBDBMNSI, RIAGENDR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}</td>\n",
       "      <td>28.467306</td>\n",
       "      <td>2127.816642</td>\n",
       "      <td>46.128263</td>\n",
       "      <td>0.168204</td>\n",
       "      <td>[RIDAGEMN, RIDEXPRG, LBDBMNSI, LBDBSESI, LBDBPBSI, LBDBCDSI, LBDTHGSI, RIAGENDR, RHQ160]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}</td>\n",
       "      <td>28.206171</td>\n",
       "      <td>2109.815688</td>\n",
       "      <td>45.932730</td>\n",
       "      <td>0.175241</td>\n",
       "      <td>[RIDAGEMN, RIDEXPRG, LBDBMNSI, LBDBPBSI, INDFMPIR, LBDBSESI, LBDTHGSI, LBDBCDSI, RIAGENDR, RHQ160]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                                   Best Params  \\\n",
       "0  XGBRegressor   {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50}   \n",
       "1  XGBRegressor   {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50}   \n",
       "2  XGBRegressor  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}   \n",
       "3  XGBRegressor  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}   \n",
       "\n",
       "         MAE           MSE        RMSE        R2  \\\n",
       "0  34.949970  31253.891986  176.787703  0.711349   \n",
       "1  34.949970  31253.891986  176.787703  0.711349   \n",
       "2  28.467306   2127.816642   46.128263  0.168204   \n",
       "3  28.206171   2109.815688   45.932730  0.175241   \n",
       "\n",
       "                                                                                    Selected Features  \n",
       "0            [RIDEXPRG, LBDBSESI, LBDTHGSI, LBDBCDSI, LBDBPBSI, RIDAGEMN, RHQ160, LBDBMNSI, RIAGENDR]  \n",
       "1            [RIDEXPRG, LBDBSESI, LBDTHGSI, LBDBCDSI, LBDBPBSI, RIDAGEMN, RHQ160, LBDBMNSI, RIAGENDR]  \n",
       "2            [RIDAGEMN, RIDEXPRG, LBDBMNSI, LBDBSESI, LBDBPBSI, LBDBCDSI, LBDTHGSI, RIAGENDR, RHQ160]  \n",
       "3  [RIDAGEMN, RIDEXPRG, LBDBMNSI, LBDBPBSI, INDFMPIR, LBDBSESI, LBDTHGSI, LBDBCDSI, RIAGENDR, RHQ160]  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAG5CAYAAABoRvUVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU3RJREFUeJzt3Qd4lFXWwPGTCgm9qiA9lNCLCAgIfCoiNnBXEVSwoCCCKysq1lV0sWLBioqFsoJdEAVFRUERpLm4BKSFKiW0AOnJfM+5yRsmlcm0zLzz/z3POMk7M29mrhPm5N5zzg1zOBwOAQAAsKnw8n4CAAAAvkSwAwAAbI1gBwAA2BrBDgAAsDWCHQAAYGsEOwAAwNYIdgAAgK0R7AAAAFuLlBC3du1a0b6KUVFR5f1UAACAizIzMyUsLEw6dep02vuG/MyOBjq+aiKt583IyPDZ+UMN4+ldjKd3MZ7exXh6l8OG41mWz++Qn9mxZnTatWvn9XOnpKRIQkKCxMXFSWxsrNfPH2oYT+9iPL2L8fQuxtO7Umw4nuvXr3f5viE/swMAAOyNYAcAANgawQ4AALA1gh0AAGBrBDsAAMDWCHYAAICtEewAAABbI9gBAAC2RrADAABsjWAHAADYGsEOAACwNYIdAABgawQ7AADA1gIu2Pnwww/l0ksvlY4dO8oll1wiM2bMkJycnPzbd+3aJXfccYd07drVXO655x45dOhQuT5nwFfSMrJk7uJNsmNfcnk/FQAIWgEV7MydO1cefvhh6dGjh7z++usycOBAmTx5srz99tvm9uPHj8uIESNk9+7d5viDDz4oy5cvl1tvvVWys7PL++kDXvfbhv0y6+uNMnvhxvJ+KgAQtCIlgHz88cfSpUsXeeihh8z3GvRs375dZs+eLbfddpt88MEHcvDgQRMU1alTx9ynRYsWMnjwYFm0aJEJjgA7SUnLMtep6bnXAIAgn9lJS0uTypUrFzhWvXp1OXr0qPl66dKl0rlz5/xAR7Vu3VoaNWokS5Ys8fvzBXzNWsLNyXGU91MBgKAVUMGOLlEtW7ZMvvjiC7NkpcHNZ599JldeeaW5fevWrdKkSZMij9NgR28D7MYKcrIJdgDAHstYV1xxhaxevVruvffe/GO9evXKX9bSAKjwzI+qVKmS7Ny50+2f63A4JCUlRbwtNTW1wDU8E4rjmZqeYa4zs7K8/h4NxfH0JcbTuxhP70q14XjqZ3dYWFjwBTtjxowxwc6ECROkQ4cO8ueff8rLL78sd955p7z22mvmhZXE1RdcnMzMTElISBBfSUxM9Nm5Q1Eojedffx031ydPpvjsPRpK4+kPjKd3MZ7elWiz8YyOjg6uYGfNmjVm2erRRx+VoUOHmmPnnnuuNGjQwCQnf//991KlShU5efJkkceeOHHC3OauqKgoiYuLE2/TCFrfWI0bN5aYmBivnz/UhOJ4/nlwu4gck+gKFSU+Pt6r5w7F8fQlxtO7GE/vSrXheG7ZssXl+wZMsLN3715zrQnIzs455xxzvXnzZpOvU9xylR4r/Liy0Fmh2NhY8RV9Y/ny/KEmlMYzPDL3V9Th8N17NJTG0x8YT+9iPL0rxkbjWZYVnYBJUG7atKm5XrVqVZEZH6UzPJq/o8tcSUlJ+bdv2LBBduzYYW4D7CYnmwRlAPBUwMzsaAn5xRdfLM8++6xZqtKcHZ2i0pwdnb7v37+/OT5r1iy58cYbZezYsaZUfcqUKeaxAwYMKO+XAHhddl6emnMXcQBAkAY76rnnnjOdk+fMmSNTp06VevXqmYaBuj2EJiHpZebMmaZ78sSJE6VChQpy/vnnm68j86b7ATuh9BwAPBdQEYIGM//4xz/MpSTNmjWT6dOn+/V5AeUlm2UsAPBYwOTsACjKCnKsoAcAUHYEO0AAy2a7CADwGMEOEAwzOyQoA4DbCHaAAEaCMgB4jmAHCIJgh2UsAHAfwQ4QFMtYBDsA4C6CHSCAUXoOAJ4j2AGCpBrLkddNGQBQNgQ7QABzntEhbwcA3EOwAwQw5wCHpSwAcA/BDhDAnAMcgh0AcA/BDhDAmNkBAM8R7AABzLlzcnY2XZQBwB0EO0AAI0EZADxHsAMEMOfdzlnGAgD3EOwAASzHqbcOMzsA4B6CHSCAUY0FAJ4j2AECWE6BZSwSlAHAHQQ7QLBUYzGzAwBuIdgBAhjVWADgOYIdIFhydpyWtAAAriPYAYKmgzI5OwDgDoIdIIBRjQUAniPYAQJYDgnKAOAxgh0gWBKUydkBALcQ7AABjO0iAMBzBDtA0OTskKAMAO4g2AECGHtjAYDnCHaAAMYyFgB4jmAHCGBUYwGA5wh2gACly1bO8Q3BDgC4h2AHCIJ8HfM9CcoA4BaCHSBAFZ7JYW8sAHAPwQ4QoApXX7GMBQDuiZQAsWLFChk+fHiJt48bN07Gjh0rY8aMke+++67I7c8//7xceumlPn6WQDnO7BDsAEBwBztt2rSRuXPnFjn+4osvyvr16/MDmY0bN8pVV10lQ4YMKXC/Ro0a+e25Av6QnV0wR4emggAQ5MFO5cqVpWPHjgWO6QzO8uXL5aWXXpImTZpIcnKy7NmzR3r27FnkvoDdl7FoKggANsvZSUtLkyeeeEL69u0rAwYMMMcSEhLMdXx8fDk/O8D3SFAGAJsHOzNmzJD9+/fLAw88kH9Mg53w8HBzm87utG3bVoYNGya///57uT5XwBfI2QEAmy1jOcvIyDABzcCBAwvk4mi+jvYaCQsLkxdeeEGOHDki06ZNM4nNmu/TqlUrt36ew+GQlJQU8bbU1NQC1/BMqI3nyZMF35Pp6RlefZ+G2nj6GuPpXYynd6XacDz1s1vjAVeEOfTeAWb+/PkyYcIE+eKLLwoEMFu2bJGkpCTp3r17/jHN4+nfv7+ce+65MnXq1DL/LE1+1uAKCDRJyZnyypf787/v07aK9GtfrVyfEwAEkujoaGnXrl1wzuwsWrRImjdvXmSmJi4uzlycVa1aVTp37mxmfdwVFRVV5LzeoBF0YmKiNG7cWGJiYrx+/lATauO5a/8JETkV7NSsWVvi4733Pg218fQ1xtO7GE/vSrXheOoEiKsCLtjJzMyUZcuWyciRI4vcNm/ePKlTp4706NGjwPH09HSpUaOG2z9Tp8FiY2PFV/SN5cvzh5pQGc/oCpkFvg+PiPDJ6w6V8fQXxtO7GE/virHReLq6hBWQCcqbNm0yEWiXLl2K3DZz5kyZNGmSZGVl5R/TJOY1a9ZIt27d/PxMAd8qXH1FgjIAuCfggh1rOapZs2ZFbtMOytu3bzfXP/30k5np0eRkXcq65ZZbyuHZAr5TuIkgwQ4A2CTYOXTokLmuVq1oImafPn3krbfekqNHj8pdd91l+vBo5+UPPvig2PsD9uqzQwdlAHBHwOXsjBo1ylxK0rt3b3MB7I6NQAHApjM7AIoPbtguAgDcQ7ADBCg6KAOAdxDsAMGyjMXeWADgFoIdIEAVTkguXJ0FAHANwQ4QLDk7gbezCwAEBYIdIEAVDm5YxgIA9xDsAAGKDsoA4B0EO0CAovQcALyDYAcIUDlFtosgQRkA3EGwAwQo+uwAgHcQ7ABBszcWwQ4AuINgBwhQVo5OdGTuryk5OwDgHoIdIMBndqKiIvK+J2cHANxBsAMEKGvZyprZIWcHANxDsAMEKGsm59TMDsEOALiDYAcIUOTsAIB3EOwAgZ6zwzIWAHiEYAcI+JkdlrEAwBMEO0DAV2PlLWNlU40FAO4g2AECPNhhZgcAPEOwAwR6NRY5OwDgEYIdIEDlWH12KD0HAI8Q7ABBUo1VeBd0AIBrCHaAAJXjKFR6zkagAOAWgh0gQFnBDTk7AOAZgh0gwBOUnauxHHmzPQAA1xHsAEHSZ0cxuQMAZUewAwRJnx3F/lgAUHYEO0CAsgIbK2fHeWkLAOA6gh0gSHY9dz4GAHAdwQ4Q8Dk7p5axqMgCgLIj2AECVHbexp+REU7LWPTaAYAyixQP/Pnnn7JkyRLZs2ePDB8+XGJjY2Xz5s1y/vnne3JaAE6zOJERYRIRHma+J2cHAPwY7EyePFlmzpxp+n6EhYXJgAED5Pjx43LnnXdKv3795KWXXpLo6GiXz7dixQoTMJVk3LhxMnbsWNm1a5c89dRTsnLlSnO8b9++MnHiRKlVq5a7LwUI6GAnIjzcKdhhZgcA/BLszJ49W2bMmCEjRoyQiy66SK6//npz/JxzzpFrr71W5syZI9OnT5fbb7/d5XO2adNG5s6dW+T4iy++KOvXr5dLL73UBFP6M6tUqWKCrZMnT8pzzz0nt956q3z00UcSEXEqtwEIdlYycni4SEREmEgWCcoA4LdgR4OZCy+8UO6//345cuRI/vGaNWvKo48+KseOHZN58+aVKdipXLmydOzYscCx7777TpYvX25miZo0aSJvvvmmHDx40ARFderUMfdp0aKFDB48WBYtWiQDBw505+UAASnHaWYnPCzMfM3MDgD4KUE5MTFRevbsWeLtPXr0kL1794on0tLS5IknnjDLVLpEppYuXSqdO3fOD3RU69atpVGjRiZ3CLATKz8nPDxMwnV6xylpGQDg42CnatWqcujQoVKDIV1q8oQuk+3fv18eeOCB/GNbt241MzyFabCjtwH2zNkJy13GYmYHAPy3jKXVVh988IH87W9/k4oVKxa47ffffze39e/f371nJCIZGRkm2NFlKQ1kLJqzo8tdhVWqVEl27tzp9s/TJOuUlBTxttTU1ALX8EyojWdmVnbudWaGhOfGOnIyJVVSUqK8cv5QG09fYzy9i/H0rlQbjqdVIOWzYGf8+PHyyy+/yBVXXCGdOnUyP2zWrFkmKVlzbKpVq2aqstyl+TeamzNy5MgCx0vb8dnVF1yczMxMSUhIEF/RmS54T6iMZ2paurnevWunZGdnma+3bdsu6cdcr3J0RaiMp78wnt7FeHpXos3G09Wqb7eCnbp168onn3wiU6ZMkcWLF5sgRJOJY2JiTHXW3XffLfXr1xdPgp3mzZtLq1atChzXpTGtwCrsxIkTHi2bRUVFSVxcnHibRtD6xmrcuLEZG3gm1MYzcqEuFWdJ0yaNpeKaE3LsZKo0bNhIWjSs7pXzh9p4+hrj6V2Mp3el2nA8t2zZ4vs+O7Vr15Ynn3zSlIBrRVZ2drapxvK0/FtnWZYtW1ZkVkdpvk5xy1V6TBOX3aWzQtoQ0Vf0jeXL84eaUBlPayIzNjZGIvN+r6KiK3j9tYfKePoL4+ldjKd3xdhoPMuyouP2dhFr1qyR0aNHS1JSkglytEJKgx/tebNx40Z3TyubNm0yEWiXLl2K3NarVy9ZvXq1+ZmWDRs2yI4dO8xtgD377DgnKFONBQBl5Vaw89tvv5nmfmvXrpXk5OT847p0pbkv2ljQ3YDHelyzZs2K3DZ06FATkd54442ycOFC+fzzz2XUqFGm/NwqTwdsWY2Vl6HM3lgA4KdgZ+rUqWZJ6ZtvvikQlNx0003y1VdfSYMGDeT555936wlZJe2a5FxYjRo1zBYVZ5xxhtkiQmeSunfvLm+//bZERnq0zRcQHMEOpecAUGaR7s6+aEVWcQGJ9uC55ppr5JVXXnHn1GamRi8l0eBKq76AkAl2InRvrNy/S9guAgD8NLOj3Vydl68K05ybrKzcUlkA7snJy8/RWR3N21Hk7ACAn4IdrXz6z3/+U2wXZd0XS/fO8qQ6CsCp/BzdF+tUsMPMDgD4ZRlr7NixMmzYMLnsssvMbuTa5VhLwLQq6uuvvzadjnXzTgDuy8mrPddKLBKUAcDPwU6bNm3k3Xfflaeeesp0TnamlVEa6LRr186DpwUg27n0nJkdAHCb2yVMukz14YcfyuHDh2XPnj0mv6BevXoFdiQH4B5NRLaaCmpysiYpW8cBAGXjcb22NhTUCwDvcZ7BofQcAMop2Pnpp59k/vz5ppuxbhVRmObwvP/++x4+PSA0OVddOVdjWRVaAAAfBztaifX444+bDUC1r46ru44CcI3zchU5OwBQDsHOjBkzzC7hb7zxhke7mwM4fbCTu4yVm7NDsAMAfuqzownJuk8VgQ7gG85BTYGNQCk9BwD/BDsa5GgvHQC+LzvX/LdTy1jk7ACAX4Kd4cOHmw059+/f787DAZyGNYNjBTmnEpSZ2QEAv+TsaG8dTUru37+/dOjQQWrVqmX2yypsypQp7pweCHnWDI4V7JCgDAB+DnacdzRfuXJlsffRqXeCHcA91gyONaPD3lgA4OdgZ+PGjR78SACnYwU1p2Z28qqxssnZAQC/5OwA8M/MjhXksIwFAOUQ7Bw7dkyefvppGTBggMnbWb58uaxdu1bGjx9vdj8H4J1qLGWVnls7oQMAfBzsHDp0SP7+97+b5oIVK1aUjIwMc/zo0aOycOFCGTJkiGzbts2dUwNwTlCOKFSNRZ8dAPBPsPPCCy+YgOfjjz+Wd955x2wbofr16ycffPCB+f7ll19259QASsvZYRkLAPwT7CxZskSuv/56iY+PN1VXzjp27CjXXXedrFq1yp1TA3DqsxOe9/tFzg4A+DnYSU5OLnWriNq1a5ucHgDusXJzrGUsOigDgJ+DnYYNG5pk5JL89NNP5j4A3GPl5uRXY1l7YzGzAwD+CXauvvpqmTdvnklQPnHihDmmy1lJSUkyadIk+fHHH2XQoEHunBpAMdVYJCgDgJ+bCo4YMUI2b94skydPlieffNIcGz16tKSnp5vkZN1G4uabb/bgaQGhreh2ESQoA4Bfgx31xBNPyJVXXimLFi2SnTt3SnZ2tsnjueCCC6RPnz5uPyEAxVVjkbMDAH4PdlTXrl3NBYB/lrGY2QEAHwU7y5Ytc+PUIr169XLrcUCoY7sIAPBzsDNy5Mgi/XScWU0FC98nISHB0+cHhKSSlrFIUAYAHwU7VhKyRSuwtIuy5uhcc8010qxZM8nJyTF7Ys2ZM0cOHz4sDz30kBtPB4DS3ycVnt9nJ3eGh72xAMBHwc7gwYMLfP/II49IgwYNZO7cuWZvLOdlKw1+tIPy4sWL5ZJLLnHjKQGwOihbMzpW0MMyFgD4qc/O119/LVdddVWBQMcSFRUlV1xxhdlSAoCXq7GyqcYCAL8EO+Hh4aVuB7Fnzx6pUKGCO6cGUEw1FgnKAODnYOe8886T9957r9gtI7755huZPXu26bcDwFvVWDQVBAC/9tm55557ZPXq1TJs2DBp0aKFNGrUSNLS0iQxMVF27doljRs3lrvvvtvtJwWEuhKrsQh2AMA/Mzv16tUze2Pdcsstpmpk6dKl8ttvv5kcnjvuuEM++eQTqV69ujunlnXr1skNN9wgHTt2NDNI9913nxw6dCj/9jFjxkjLli2LXBYsWODWzwMCuhqrSIIyOTsA4LcOyhrMTJgwwVy85Y8//pDhw4dLjx495JVXXpEDBw7I888/bwIoLWlXGzduNMnRQ4YMKfBYnV0C7J+gzMwOAPh1uwjd+PPo0aNmX6ySZoDK4tlnnzWzNK+99ppERESYY5UrV5Z///vfpodPjRo1TPJzz549zcwPYPtgJ4IOygBQLsGOBjiPPfaYfPvttyUGOmXtoHzkyBFZuXKlCWysQEfpDup6UStWrDDX8fHx7jxtIGhYMzh5MQ4JygDg72Dn6aefNr12dBPQ1q1bS3R0tHhq06ZNJk+hVq1aJgFamxIqrep6+OGHpVq1aiZ40rL3GTNmmNu1/L19+/Ymr6dDhw5u/2zd7iIlJUW8LTU1tcA1PBNK45menmGuHTnZ5r2ZkZFuvtc/Lrz1Xg2l8fQHxtO7GE/vSrXheOpnd2lbWXkc7Hz//fcyaNAgeeqpp8RbdIsJpdtMnH/++WYpS5euNGfn1ltvNTk7mq+jAZG+ON2uQmeDpk2bZvJ8tJtzq1at3PrZmZmZPt3HS6vU4D2hMJ4HknL7WB09esS8N/cfzTTfp2dkef29Ggrj6U+Mp3cxnt6VaLPxdHWyxa1gJyMjQ7p06SLepAGH0pkiXcpSmqhcpUoV+ec//2kqvnRDUg2yunfvnv84vY8uc2lwNHXqVLd+tnZ9jouLE2/TCFrfWFqKHxMT4/Xzh5pQGs9VO/4UkeNSp3ZtiY9vLlUPnhSR/RIeHuG1ZdxQGk9/YDy9i/H0rlQbjueWLVtcvq9bwY4uGWmJ+NVXXy3eUqlSJXOtszrOevfuba43bNggffr0KRKUVK1aVTp37mxmfdylM0WxsbHiK/rG8uX5Q00ojKcGNapChSjzWitVysnfCNTbrz0UxtOfGE/vYjy9K8ZG4+nqEpbbfXYmTpwo3333nUyfPl327t1rqrJ0tqfwpSw02nSe4bFkZWWZa+3ho719li9fXuSx+vO1UguwX+k5HZQBwFNuzexoYz8NQp577jlzKSni0tkYVzVr1kzq169vmgOOGDEiP2L74YcfzLUumz3++ONy4sQJmT9/vkRG5j71/fv3y5o1a0wjQsD2e2PRZwcA/BPsnHvuuWWaPnKFnu/ee++Vu+66S/7xj3+YpoHbt283CcoXXnihqboaO3asjBo1ylzrVhVaAv/qq6+apSzt5gzYhbW7edHtIuigDAB+CXa8WYXlbMCAAfL666+bAGb06NGm3FyDnvHjx5vbNWfnrbfeMrdrUKSzO7169TJdnPW+gF1obo5zkGPN8OiEj+6PZX0PAPBxB2VdytItHjRvR2d7NK9G+4B4Enj069fPXEqiCctW0jJg/w7KYQU6KVuBULgQ7ACATxOU1TfffGOCkqFDh5odzjdv3mx2QtfZl3feecfd0wLQgMbqoFxoGUuRpAwAfgh2tCJKl5F07ytdYtIuhuqss84yVVW6x9WXX37pzqkBFFuNFVYknwcA4MNgRxv4abfi2bNnF+i106JFC/nwww+lbdu28v7777tzagAm2Ck+QVlpzg4AwMfBjubpXHHFFfnl34VbN2uX423btrlzagDFlJ47JySzjAUAfgh2dDPO0mgvHG+XpgOhxJq9sWZ09PfJineY2QEAPwQ7nTp1ks8//7zYnh/JyclmU05PdiEHQt2pnJ2wIn9kMLMDAH4Idu68807ZunWr6YGjgY3+1blq1Sp588035fLLL5cDBw7I7bff7s6pARSY2Tn1K2qVoRPsAIAfgh3tZqyBjXYwfvHFF001ljb6027H+rUeO+ecc9w5NQDnnJ28AKfAlhF0UQYA/zQV7NGjh+m1k5CQIDt27DBLWrq3lVZiFZe4DMD97SKcv2Z/LAAoG4+iEl2+qlWrlqSmppoAp0GDBgQ6gA+qsZyXtEhQBoCycTsy+fXXX+WZZ54xMzvOwY9uG3H//fdLy5Yt3T01EPIK743lHPiQswMAfgh2VqxYISNHjpSYmBiz+7h2TdY9sRITE2X+/Pnm2AcffGCaDAIoO2upqsAyVn6CMjk7AODzYOell14yW0PMmTPHLGM5GzNmjKnS0mTlN954w53TAyGv8HYRuV8zswMAfqvG0qUrnb0pHOiounXrms1BtRQdgHusHlYFc3ZIUAYAvwU71apVM12SS6Ll5xUrVnTrCQFwmtmJKNpUkARlAPBDsHPjjTfKe++9J2vWrCly2/bt22XmzJly0003uXNqAM7VWE7brtBnBwD8mLNz7NgxM7tz3XXXmeqruLg4U3K+c+dOWbp0qfl6w4YNcvfddxd43JQpU9x8mkBo743lvKRFrAMAfgh2Xn/99QKVWXpxlpWVJQsWLChwTMvSCXaAsi5jFZegTLQDAD4PdjZu3OjOwwB4UnpONRYA+C9nB4B/qrEK9tlh13MA8Guwo3k7Tz/9tAwYMEA6dOggy5cvl7Vr18r48ePNXlkAvL1dRF7ODqXnAOD7YOfQoUPy97//XWbMmGFKzDMyMsxx3QV94cKFpqngtm3b3Dk1gBKCnVPbRZCzAwA+D3ZeeOEFE/B8/PHH8s4775i+Oqpfv35mmwj9/uWXX3bn1ABKqMYiZwcA/BjsLFmyRK6//nqJj483VVbOOnbsaErS6aAMuEf/WCh+uwhydgDAb8FOcnKy1K9fv8Tba9eubXJ6AJSdcyzj3EH51EagBDsA4PNgp2HDhiYZuSQ//fSTuQ8A9yuxSmwqmE3ODgD4PNi5+uqrZd68eSZB2dojS5ezkpKSZNKkSfLjjz/KoEGD3Dk1EPKcN/osdiNQZnYAwPdNBUeMGCGbN2+WyZMny5NPPmmOjR49WtLT002+Qf/+/eXmm29259RAyHMOZoqb2SHYAQA/BDvqiSeekCuvvFIWLVpk9sTKzs42eTwXXHCB9OnTx93TAiEvJ6+60Xmn8wJ9dgh2AMA/wY7q2rWruQDw0TKWU7Ej1VgA4MNgZ9myZW6dvFevXm49DghlVtNAnclxbu1Azg4A+DDYGTlyZJF+Os6spoKF75OQkODm0wJC16keOwV/n9j1HAB8GOxYScgWrcDSLsqao3PNNddIs2bNTLms7ok1Z84cOXz4sDz00ENuPiUgtOV3T3bqsaPC874nZwcAfBDsDB48uMD3jzzyiDRo0EDmzp1r9sZyXrbS4Ec7KC9evFguueSSMj4dkXXr1smUKVNk/fr1EhsbK71795Z7771XatWqZW7ftWuXPPXUU7Jy5Urzfd++fWXixIn5twO22Rer0Expfs4OG4ECgO/77Hz99ddy1VVXFQh0LFFRUXLFFVeYLSXK6o8//pDhw4ebIOeVV16RCRMmyM8//yx33HGHuf348eOm7H337t2m7P3BBx80u63feuutphoMsANr5sa5EkuRswMAfqzG0n+ES9sOYs+ePVKhQoUyn/fZZ5+Vli1bymuvvSYRERHmWOXKleXf//63WSLTMveDBw+aGaU6deqY21u0aGFmnvS2gQMHuvNygMDM2Sm0jEXODgD4cWbnvPPOk/fee6/YLSO++eYbmT17tum3UxZHjhwxS1NDhw7ND3SUNijUjsyNGjWSpUuXSufOnfMDHdW6dWtzmzszSUAgys7bDqLkBGVmdgCgLNya2bnnnntk9erVMmzYMDOzosFGWlqaJCYmmpyaxo0by913312mc27atMkkOWvujZ5fc36UBk0PP/ywVKtWTbZu3WqCn8L05+ttgJ2rsUhQBgA/Bjv16tUze2O9/fbbZtZFZ1yUJi1rfo1uFaF5N2WhFVxKq7jOP/98s5SlS1fPP/+8ycnRKi/N2dFlrcIqVapkuji7S0vnU1JSxNtSU1MLXMMzoTKeKSm5r0/zk53flzlZWeY6PSPTK+/XUBlPf2E8vYvx9K5UG46nfnaX1hbHKx2Uq1evbhKI9eINmZmZ+ctSmqOjevToIVWqVJF//vOfJqCy+vkUx9UXXNLP9mVPIJ3xgvfYfTx3HEg311mF3pdJScfN9eHDR736frX7ePob4+ldjKd3JdpsPKOjo32/XYQ36eyM0lkdZ1p6rjZs2GACn5MnTxZ5rPb90dvcpRVkcXFx4m0aQesbS5f1YmJivH7+UBMq45ldQWc5D0pMxQoSHx+ff3zbkR0ickyqVK1a4Li7QmU8/YXx9C7G07tSbTieW7Zscfm+ARPs6P8A5xkeS1be1L2WuTdp0qTY5So9ponL7tJZobIuu5WFvrF8ef5QY/fxjI4+Ya4jIyMKvE4NflRYWLhXX7/dx9PfGE/vYjy9K8ZG41mWFR23qrF8Qbswa0fmBQsWFFiu+uGHH8x1ly5dTNNCTYxOSkrKv11nfDS3h324EDql5yQoA0BZhAdShKadkrVz8j/+8Q/TTHDWrFkmf+fCCy+U9u3bm7J0jUhvvPFGWbhwoXz++ecyatQok+czYMCA8n4JgG+rsfKaDFKNBQBBGuwoDVhef/112bt3r4wePVreeOMNGTJkiNmHS9WoUUNmzpwpZ5xxhtkiQvfs6t69u6kKi4wMmBU5wCPWdhDW9hAWZnYAwD0BFyH069fPXEpb7po+fbpfnxNQPttFFFrGylvWspoOAgC8GOy4kw+jy1JW/x0ArrO2g6CDMgD4MdjRKigA5TyzY+16TrADAN4PdjRPBkB5JyizXQQABFSC8l9//eWrUwMhEuyEF5+zw67nAOD7BGXtg/P++++bnBzdo8e5L442AdQux9r7RnvgACgb+uwAQAAEO2+99ZbZoFP3pNCNOY8cOSJnnnmmHD161LSk1m7H2gsHQNnl5FVbFc3ZYRkLAPy2jPXZZ5+ZvXl++eUXsxu5zuzMmDFDVq1aJQ8//LCkpaVJx44d3XpCQKjLn9kp1ArdCn6Y2QEAPwQ7e/bskSuvvNLM6jRs2FCqVq1qtnGIiIiQ6667zjQH1GUuAGWXk7csHB5RQjVWXtNBAIAPgx0NaqxdypUGPJs2bcr/vkePHrbbRh7wfwdllrEAoNyCHe2745x8rDuWJyQk5H+veTuauAzAe9VYp5axqMYCAJ8HO5deeqnJ1Zk8ebLJz+ndu7esXLlSPvjgA3Ot+Tu6rQMA7/XZoRoLAPwY7GillebszJ4923x/2WWXSdu2beWxxx6TESNGyIEDB2TcuHFuPiUgtJW4XUQEHZQBwG+l55qz89RTT8k999xjyszVrFmz5KuvvjLl57qXVlxcnFtPCAh1JW8XYW0ESrADAH7b9bxWrVr5X2vPnUGDBnlyOgAuBDs55OwAgPeDnbvvvtuUlHfu3Dn/e1dMmTKlbM8GwGn3xmIZCwB8EOwsWLBA+vbtmx/s6PenExYWRrADeLRdRKG9sdj1HAB8F+xs3Lix1O8BeE92dkkJyszsAIDfqrE+//xz2b17d4m3b926VaZNm+bWEwJC3elKzzWnx3nzXQCAD4Kd+++/X9atW1fi7b/++qu8+uqr7pwaCHklJSg7f8/kDgB4eRlr165dpszcon9Vvvzyy6bcvDCtFNmyZYvUqVOnDE8DgKszO9bvWUR4hN+fGwDYNthp0KCBtGjRQpYtW5affJycnCyZmZnF9uBp3rw5TQUBj2d2it8uwuq1E+VR4wgACB0u/3M5adKk/K9btWolDzzwgFx++eW+el5AyCp5ZudU8EOSMgD4OGfnmmuukapVq7rzUACubheRV31lcQ5+CHYAwA/VWDt37nTnoQBOw9oOorimgmF5h9j5HAB8HOycddZZsm/fPnceCuA0rFmbcCuyKaH8HADgGrdSHMePHy8PPfSQJCUlSbdu3cweWZqYXJhuCAqgbHLyeugUXsbKT1rOzmYzUADwdbBz1113mesvvvjCXLQ6y5mWpuuxhIQEd04PhLSc7OKrsQrsfM7MDgD4Nth58skn3XkYAA+qsZyPkbMDAD4OdgYPHuzOwwCUpRqruGCH/bEAwD8Jys6ysrIkIyMj/5KammpyeebNm+fpqYGQ5MrMDgnKAODjmZ1jx46Z/bG0o3JxXZQtV1xxhTunB0JafjVWMcGOlcfDzA4A+HhmZ8qUKfL9999L69at5bzzzjMJydpNuUePHhIZGSkVKlSQV155xZ1TAyHPmrVx7phssQIgZnYAwMfBzo8//igXXnihzJkzR5599llzbPjw4fLOO++YY1qJtW3bNndODYS8HFcSlCk9BwDfBjuHDh2Snj17mq9r1qwpdevWld9//91836ZNG/n73/8uX375pTunBkKelaAcXkyfHaqxAMBPOTuxsbEFvm/YsKFs3rw5//uWLVuaLSXc0bVrV7OjemGaH1SnTh0ZM2aMfPfdd0Vuf/755+XSSy9162cCwbBdhPMxcnYAwMfBjubqLF68WIYOHWq+b9q0qaxduzb/9t27dxfbEO109HEa6Gh35nbt2hW4rXr16uZ648aNctVVV8mQIUMK3N6oUSN3XgoQZNVYJCgDgF+Cneuvv17Gjh1rgo73339fLrvsMvnwww/l7rvvliZNmphj5557bpnPq4GMuvjii83SWGEaCO3Zs8csoXXs2NGdpw4EdzVW3tIWCcoA4OOcHU1Onjx5sumro0taGtjccMMNsmDBAlOFpXk89913X5nPq9tL6D5bxQU61u0qPj7enacNBH011qkEZXJ2AMCnMztKZ3X0YnnwwQflpptuMj144uLiJCoqqszn1GCmSpUqcvvtt8uKFStMSXvfvn1NTx8NgPR2XR6bMWOGWUbTn9W+fXsTWHXo0MHdl2J+TkpKinibNlh0voZnQmU8rUAmIyO9yPsyTHIDoZTUNI/fs6Eynv7CeHoX4+ldqTYcT2sfTq8HO9oZed26daZrslZdNWjQoMDt9erVMxd36TLWkSNH5Oqrr5ZbbrlFtmzZIi+//LKZNfr000/N7Tk5OebFvfDCC+a+06ZNM2Xvc+fOlVatWrn1c7Uxoi83LU1MTPTZuUOR3cczIzPLXCdu3yYnDxf8oyEt7x+qnbt2S9Xww175eXYfT39jPL2L8fSuRJuNZ3R0tPeCHY2enn76aZk1a5ZkZ2fnH9fcGl3OKlyd5a5nnnlGKleunB+0nHPOOdK8eXMZNmyYfPbZZzJy5EgZNGiQdO/ePf8x2siwf//+8tprr8nUqVPd+rk6C6WzUd6mEbS+sRo3biwxMTFeP3+oCZnxDNuni1nSonmcnFmr4O9WlRUpIgfSzR8V8fFnefRjQmY8/YTx9C7G07tSbTieOiHiKpeCnZkzZ8p7771nlowuueQSs5T0888/y8KFC82geWsXdA1uCuvSpYtZ2tJZHU2MLhyUVK1aVTp37pyf3OwOnSnyVsBWHB0jX54/1Nh9PHMcuUtVlSrFFnmdUVG5v7IRkdFeGwO7j6e/MZ7exXh6V4yNxtPVJSyXE5R1VkUroLQ7subljBgxQt5880259tprZf78+ZKWliae0iWpjz76qEjnZV220mWmGjVqmM1Fly9fXuSx6enp5nbATn12wsNK2wiUBGUA8Gqwo1NfF110UZHeOdopWfN3vLE1hC4lPfroozJ9+vQCx3UPLg2munXrZmaYJk2aZH6mZf/+/bJmzRpzO2AHViATUUwHZSsAos8OAIh3l7E02Chu2uuss3JzBo4fPy6e0lwdnTV6++23TQPBXr16yaZNm0yCslZk6feaLzRq1CjT40fzeI4ePSqvvvqqWcrShGYg2Gl+nBXHFNtUMC8AItgBANe5nKBc3NqYNdPjrSn18ePHmxJzrazSWRxdmtIuzRrcqD59+shbb71lApy77rrL7LCuQdCECROkWrVqXnkOQHlybhZYagdlNgIFAN/32fGFiIgIU0aul5L07t3bXAA7cp6xKa6DMntjAYAPgx1dMtq7d2+BY9rUTx0+fLjIbcqTnjtAKHIOYiIiiqbUWQEQCcoA4INgR/vp6KU4uoxUmC57bdiwoQxPBUCBmZ1SqrGY2QEALwc7gwcPLsMpAbjLec+r4hOU2fUcAHwS7HiraSAA1xoK6qROqTk7JCgDgG93PQfg6x3Pi+8MemoZi5wdAHAVwQ4QiN2TCzXwLJqgzMwOALiKYAcIINkuz+wQ7ACAqwh2gABiLU+VGOzkJSgzswMAriPYAQKINWNTXHKyOc7eWABQZgQ7QDAlKLM3FgCUGcEOEIw5O079eAAApSPYAQJwZie8mK0iFAnKAFB2BDtAAJaelzSzY5Wkk6AMAK4j2LEJh8NhLrB5NRYzOwBQZgQ7NvHm5+tl6MNfy6FjqeX9VODDaqxTCcrk7ACAqwh2bCAtI0u++XWHnEzNlI07jpT304E/totgbywAcBnBjg2s35IkGVm5f+mnpGaW99OBB+igDADeR7BjA78l7M//+mQawY49ZnbCS09QJj8LAFxGsBPkNCl5lXOwk5pVrs8HnrFycUrM2bE2AmUZCwBcRrAT5HbuOy4Hj5xKSk5hZscey1h5iciF0UEZAMqOYMdGS1iKZazgZiUeW3tglbw3FtVYAOAqgp0gZy1hNT6rqrnWiiwELysXhwRlAPAegp0gdiIlQxISD5uvz+9U31ynpJGzY4sOyiVtF5F3nGAHAFxHsBPE1m46aKp3GpxRRZrUq2aOsYxl79JzK3GZBGUAcB3BThD7LWGfue4af4ZUqhhlvk6hGiuo5bhYjUXODgC4jmAniGcAVm88YL4+p/UZEhsTab4+Qc5OUKOpIAB4H8FOkNq864gkn8yQShUjJb5xzVMzO2mZbAhq572x8poKEuwAgOsIdoLUqg25VVidWtaVyIhwia0Ymf8hmJ6ZXc7PDj7bG4s+OwBQZgQ7Qd5fp2vrM8x1TIVIsT4fqciywzJW+GkSlMnZAQBXEewEoUPHUmXbnmOi/eW6tMoNdsLCwiQ2bymLXjvBy0o8LrGDshXssFQJAC4j2AlC/9t2yFzHnV1dqlWukH88NiYv2KH8PGhZJeWnr8Yi2AEAVxHsBKGko7l7YdWvW7nAcU1WVpSf27gay2oqSJ8dAHAZwU4QOnQszVzXqlqxwPH8ZSxmdmxcjcXMDgCUVe5UQADp2rWrJCcnFzm+bNkyqVOnjuzatUueeuopWblypTnet29fmThxotSqVUtCxaHk3GCnZrWCwU7lvGUsdj63QzVW+Gk2AiXYAYCgDHZ2795tAp2HHnpI2rVrV+C26tWry/Hjx2XEiBFSpUoVmTx5spw8eVKee+45ufXWW+Wjjz6SiIgICQWH82d2Ygoct8rPSVC28zJWXoIyHZQBIDiDnY0bN5rriy++WOrWrVvk9nfffVcOHjwoc+fONbM8qkWLFjJ48GBZtGiRDBw4UEJqZqfQMpbVWPAkpefBX411mr2xmNkBgCDN2UlISDDLUcUFOmrp0qXSuXPn/EBHtW7dWho1aiRLliyRUKDdka2ZncLLWFY1VgozOzbeLiL3V1Yrz60lLwBAkAU7ukR1++23m6CmU6dOMn78eDlwIHcPqK1bt0qTJk2KPE6DHb0tFBxPyZSsvIZyNaueKjsvOLNDsBOsrAAm/DR9dhSzOwAQpMtYR44ckauvvlpuueUW2bJli7z88styww03yKeffmpydipXLlhurSpVqiQ7d+70aLYkJSVFvC01NbXAtTfs3X/cXFeJjZLMjHTJzDh1W1REbhCUfCLNJ6+nvPliPANNekZuoJqdlVXs/8P0jFNbgZw4cVIqRLufpxYK4+lPjKd3MZ7elWrD8dTPbm2oG3TBzjPPPGOCmVatWpnvzznnHGnevLkMGzZMPvvss1I3uHT1BRcnMzPTzCr5SmJiotfOtXlv7hJWbHTuTJizw0m5H45JR5J9+nrKmzfHM9AcOXLUXCclHZSEhNz/186ynPrrbNi4USpGeT45a+fxLA+Mp3cxnt6VaLPxjI6ODr5gR4Obwrp06WKWtnTWR6+1AquwEydOmNvcFRUVJXFxceJtGkHrG6tx48YSE1Owcspd+1L26EehnFWnqsTHxxe4LT0ySeTnwyLh0UVuswNfjGeg+faP9Zp1JWedeYbExzcqYZlL3wMizZu3yG834I5QGE9/Yjy9i/H0rlQbjqeu/rgqYIIdXb5avHixCW6aNm2af1xLbHXmpUaNGiZfp7jlKj2mOT7uMvtKxcaKr+gby1vnP5GWu1RVp0alIuesWT13iS81I8enr6e8eXM8A01YWO5MTUzFCsW+RufZzQoVKkpsbMG8LXfYeTzLA+PpXYynd8XYaDzLsqITMAnKOrvy6KOPyvTp0wsc//777yUtLU26desmvXr1ktWrV0tSUlL+7Rs2bJAdO3aY20Kpe3LhSqwCCcpUY9m2Gkt/uemiDABlEzAzO5qrc9NNN8nbb79tGghq8LJp0yaToKxdkvX7Nm3ayKxZs+TGG2+UsWPHmiBoypQppvx8wIABEgoO5/XYqVWt6DRkpbwljdS0zDIlbiEAq7FK6KCsNNjRQIf9sQAgyIIdpWXm2mNHmwbOnDnTLF0NHTrUBDZKv9fj2j1Zt4ioUKGCnH/++ebryMiAeik+byhYeF8s5w7K+nmZmp6Vv1cWgsepmZ2S72O6KGedakAIAChdQEUIut3D8OHDzaUkzZo1K7LUFUryGwoWE+xUiIrI/6s/JY1gJxhl5/VQKm1mx9ofi6aCABBkOTtw7YPw6PGSc3Z02cpayiJvJzjlOErP2XEOhMjZAQDXEOwEkaMn0s0Sle6PVK1y8VU4dFG2yTJWCR2UnW8j2AEA1xDsBGFyco0qFUr8yz82JndlUpexEHyspOPSZnbyq7HylrwAAEGUswMXy86LydexUH4e3Kw8HGvDz+K4W3r+584jcuDIqS0o0tMzZM+eFDmavV8qVHCtC6mlSky0NG9YnbwwAEGBYCcoy85LCXasnc9ZxgpKVoWVLlWWxAqEXE1Q3n84Rd7+Yr38+se+Eu5x2I1nqjliIo3OrCrxjWtKq8Y1pLLuYWJDVStFS8uGNWjlAAQxgh2bVGIVLj8/wcxOULJma0oLdqzbMrKyTQB87ES6JJ/MMIFuvdqV8mdbMjKz5ZMftsjH3/0pGVk55nGtGtXIf3xOdo6cTDkplWIrSXhpte7FOHgk1QRRiX8lm8vXy+21305hvTvWl3HXdJSYCvyTCQQjfnODcGanuEqswstY5OwE+zLW6ROUH3z9l2Jv15yuenUqy8GjqXLgcO6yVbtmtWXUVe3MTIxFd1XXDWN1HzV32sfr+3Fj4mFJSDwsm3cdlcysUzuy24UWx23bc0yWrtsjiX8dk/tHnCsNznB/Hz4A5YNgJwhzdoprKGix/qqnGsue20WoxmdWlZ37jpuv9W5VKkVLldhoM7ujlyPH083FWvK85fK20qtjPa8vw+gM43nt65mLnSVsPyxPzfhNdu0/IXe/9KOMu6aTmekBEDwIdoJyZqfkHWsrWdVYqczsBHewU/Ky0j+v6yLDBrQyO55rnoxzYKTLl3sPnpC9SSclKytbenaoz9KLh+Kb1JSX/tlXnp21Sv67JUmemblKPl2yRaIKLf1l52RLakqqxCw7LhHhET57PhqzXty9sfzfOQ189jMAu+FfQZvN7NBnJwT67ISHSf06uTvcF6YBUIuGNcwF3lO9SgWZdFsPmfl1gsmD2rLraCn3zvD589Gltc4t65rnBeD0CHaChCabHk/JOG3OTiwdlIOaJg07bwmBwBERES43XtZG+nQ+W/YdOlnkdi3l3717t5x99tllLuUviznf/mmCnU9+2Cy3XNHWZz8HsBOCnSBbwoqODDd/vZekUl41ViCWnq/784B8v2qXFN6/8qzalaR9XG1p2aiGREf5bvq/8AzKT2t3y5fLtslJF5b89Dm2a1ZL2jarLc3qVzMffL56Xqeb2UH5alKvmrkUpgnfVcIOSXx8XbcSvl2lvyOPvvWrfPXzdhncN67U6kwAuQh2grASq7RE01MJyoGVs7Nm4wGZNP3XEhvhzfl2k0RFhpueLRr0REUWDHrOqBkjrRp4XgXjcDjkt4T9MvOrBFMy7ao9B0/IqoT95uuYChESd7b2lYkym69WiI4w103qVZVeHepLxWJyZHbtPy5f/bJd/koqOiPgzJq9K630HKFNl6+0hcDGHUfk4+83y22D2pX3UwICHsFOsAU7p/krrnIALmNt3nVEnnx/pQl0zm19prSLq51/W05OjmzdfUzWb00yFUSaAKqXkpxZI0q6746ULvFnFejeq0FMWka26Tlz7ESGudbAITIi3PwlrAFJdGSE/PrHX6ZU2poFu6pfc2ndpGapz1+fty4b/G/bIXPRJGB9vsV58/M/zDLHxd0bSdN61WT1xv0yf+k2WfvnQZfHy+x9VolcDBRP/9i5fkC8PDTtF/n6l0S5qm+c1K5ectECAIKd4EtOLqUSS1kBQGp6lvmQLq2E2R90JmPS2ytMINKxeR2ZOKKrmcEpTIOV3QdOyB9bk8yMS97m3/k7gW/dfVS27D4m+45kyuc/JZqLu3Qp8PLeTeVv/9fclGy7okPzOmbJQPvg7NiX20hPX1O6uWSZmTQNpPT1LlyeaC5VYqPkeEpu0KmTcd3anGmCvdPN2mgfFxJPUZr2zWtLm6a1TPD94Xd/ypi/dSjvpwSb+W3DPnn90//K8ZOeJ9zrv3+X9Woqwwe2lvJCsGOj7snOpedWwFNafo+vHT2eLv96c7nZrb1p/Wpy/43FBzrWX6v6IV9aw7a/DhyVr3/6rxw8WUG27j1eZEmsQlS42Q1eL9UrVzDLTBqcpGfmBiWa5F2jSkW54vympw0aS6KBSkk5Gzde2lr+2JYki5bvkF/W/2UCHZ09uqhbI7m0ZxM5s1Ylt34mUNzvy3UDWskDr/0s367YIX/v11zq1vRdnhBCy+qN+2Xye79Jlhc3G957sPQlfF8j2LHZMpbmumhAkZmVIympmT4JdnYfOC4Lft4uaenZxQYD5hIm5q/Ovw6dlDNqxsqjI7t7vGlktcrR0qFJJbc7/vqavu72cXXMRZfREvcmS4tGNehzA5/QrtgdmteW3zcnmZy3O4d0Ku+nBBv4/c+DMvndlSbQOa/9WXLTZW28EpzXrVG+S638KxxsO56XUnZu0T2SdFbF2712srNz5LMft8p/Fm00wZSrmyg+dlsPqRFiFSM6u9ShRZ3yfhqwuesujpffNy+V737bmZ+LVpKK0REmICpuVhJQ+gfq4++uMHvp6ZL7hOvOKXE2PtgQ7ASJw8mpp93x3KJLJybY8WKS8va9x2Tq3LUmb0Z1bKEzGKcSjS26bKSXbIdDIsLCpE+Xs6Ve7eIb4AHwvLuz5oKt+N8+k/N2Oq9+9Ls8M6431X5epJWWWq1ZnsLyZk80Nyb/2hw9JS09TXbtTZP0yCSpWKHo50jyyXR57ZPfzZJ/51Z1ZeII+wQ6imAnyJaxSuuebIn1cDNQncE5cCTV/ALr1gOajKv9cTRHRpfFRl7Z1rSq9/ZeSwDK7p4bzjEJ/NYmssXRv9Sfen+lbNp5RH5cu1v6dWGrCW/Ysvuo3DN1qVdzW3wvqdRbdWn0gRvPLdL+I9gR7AQBbRCYmpcf40oDMU+2jFi+fq88N3uNSeYtrEe7s+T2q9qH3JIUEMi0x1PrJrVOe7+rL2ghM75KkPcXbJAebc8qth8Uyvbv8rMzV5lAR/MSa5RjBaUjr6JV4129dhQTe+Xk5JjZHZ3VCS9h771mZ1czfZv0PWU3vNuDKF9Hl6dc+QcqNn8z0LIHOz+s3m0CHS3PrlensukcrPswaTKkTm0CCE5Xnt9MFv26Q/YfTjH7e2k1F9w37bP1ZsNd7XH0wvg+LrexKC8pKSmSkJAQsAUevkawE2Tdk11hzeyccGNmR6dl1aO39TABDgB70OaaN13eRp56/zf59IfNclG3hlK3Ruh96HnDD6t3maV9TX2acF2XgA90IGKf7CMbc7Xs3LkaS6W4sOeTMy2XPngkNxFau/8CsJfz2p1lmhFqDo8uZ6HsNI/x9U9+N19f27+VGU8EPmZ2gqns3MVg59T+WGWb2dEtEVS92pXyAyYA9qFFBbde2VbGv/ij/LR2j1zWs6mp6LIjzV3RooqsrBzJynGY7z2lSeDPzlplcig1yLnmwhZeea7wPYKdYKrEcrHr76mdz8s2s7M1L9hpdnb1Mj9HAMFBf78v7NpQvl25U/711i9SKSYwl2A0OMnMzJSI+QdEi500ETgr22GqRU/72LzAxFd0Kxhdvirv7XjgOoIdG20VYbFmZcraZ8fK12lWnyUswM5uGBhv9nLTLU1S03OXrgNX0crQ8s59Gj+0M5uvBhmCHRsmKLu9jJXXMDCOmR3A1nSPuNfvu0AOHEmRQJWWlibbtydKs6ZNpHLlWImKCJeIiHCJjHBtNiUiPPe+uY8J91ojRT0LTRmDD8FOANNp3B/X7JZte4+53FDQeTNQ7QPhqhOpmWYfK9X0bGZ2ALuzNs0N5FLpjORoaVKvakiWSsO7CHYC1L5DJ+X1T/4razYdMN+3bFhD4hpUL9vMThmqsbbtyV3C0p2TKaMEANgJwU6A0eqBeT9tldmLNpo9SnRvkmsvaimD+8aZqVhXuNNBeWv+EhazOgAAeyHYCSBaPfDSnDWmi7HSpn53XN3BdDAuCytBWYMlrWBwJUiygp1m9cnXAQDYC8FOAOXn6I6zGuho8tvoq9rLgO6N3NpsMzav9NwqP69aKdr1SixmdgAANkMH5QAJdN7+4g+zb43GNncP6yyX9Gjs9q7iOpNTITrC5SRlvc/epBPma2Z2AAB2E9DBzhNPPCEtW7aUrKxTibZjxowxxwpfFixYIMFq5tcJMm/pNvP1ndd0lPM7ne3xOa3GglpldTrb9yaLNhetXa2iVC/HnXsBAAipZazly5fLrFmzihzfuHGjXHXVVTJkyJACxxs1aiSB5qd1e2XV+qOyYttGiYwqfvuFo8fTZem6PeZrXbq68FzvvA7N2zmcnO7SzM7WvEosOicDAOwoIIOd5ORkmThxopx55pny119/FTi+Z88e6dmzp3Ts2FECWVpGlrz+2Ya8luW5S0SlufnyNnJpzyZe+/llKT8/lZxMvg4AwH4CMth57LHHpEGDBtK1a1d57bXX8o8nJCSY6/j4eAl0FaMj5a5r2slv67dL7Vq1JTKq5KHWHjrd2p7l1Z9vlZ+7NLNjJSe72McHAIBgEnDBzldffSXff/+9zJs3Tz7//PMCt2mwEx4eLjNmzJDFixfLsWPHpH379nLfffdJhw4dPEoQ1m6d3ta+aVWpGl5NGjeuLzExpe+j4u2fXyEqN7n5aHJKqefW8vSd+4+br+vVjPbJOHhLampqgWt4hvH0LsbTuxhP70q14XjqZ7erhTwBFezs37/fzOrce++9ZmanuHydnJwc8+JeeOEFOXLkiEybNk2GDx8uc+fOlVatWrn1c3VnXWvWyBcSExPF3zLScpfOEnf9JQnVc7eBKM6upHSTnFypYrjs271N9rtZAeZP5TGedsZ4ehfj6V2Mp3cl2mw8o6Ojgy/YeeCBB6Rt27YydOjQYm8fOXKkDBo0SLp3755/rEePHtK/f3+z3DV16lS3fm5UVJTExcWJt2kErW+sxo0bn3Zmx9tW7/xTVm85KZWqVJf4+JYl3m/nil0iclBaNKwprVu3lkBWnuNpR4yndzGe3sV4eleqDcdzy5YtLt83YIKd2bNny7p16+SLL77ILzXXWRzrWi8akBQOSqpWrSqdO3c2sz7u0pkiX240p28sf29kV61K7ps5I0tK/dm7DuQuW2mwEyyb7ZXHeNoZ4+ldjKd3MZ7eFWOj8SxLL7qACXYWLlwoJ06ckAsuuKDIbe3atZOxY8ea8vI6deqY2Rxn6enpUqNGDT8+28B3KkE5y7VKLDonAwBsKmCCHc3VOXmyYG7Jhx9+mH/RMnQNeDQgmj9/vkRGRubn+axZs0ZuuOGGcnrmgelU6XlmgWSuYycyZNf+4yYpWa937Es2t9FjBwBgVwET7DRt2rTIsSVLlpjrNm3amOBGg51Ro0aZ62HDhsnRo0fl1VdfNUtZt9xyiwQSrWpq3ry5+fr3338vdtrQ+T6bN28ucJ/Sbjvdz9P7V87bDHTz7qNy78tL5XBymhxJTpOMrNylQUt2Vrr8/vF46TTHtZ9T1tfurXMG8s8N9eceygr/ngMITAET7LiiT58+8tZbb5kA56677jIBUK9evWTChAlSrRrLMM5qV4/Jn9lJSDycf1yXOM+oGSsNzqgiDc+oInWrR8qtH5fjEwUAIJSDnXHjxpmLs969e5sLSte0fjWZcF0XOXYyXWpVjZEaVStIzaoVzSU6KneTUBXIfXUAALB9sAPP9Ons+YaiAAAEu4De9RwAAMBTBDsAAMDWCHYAAICtEewAAABbI9gBAAC2RrADAABsjWAHAADYGsEOAACwNYIdAABgawQ7AADA1gh2AACArRHsAAAAWyPYAQAAthbmcDgcEsLWrFkjOgTR0dFePa+ec9euXebrs88+W8LDw0u9T4MGDSQsLMyl207381y5v6eP8/c5rfNmZmZKVFRUsef01c/1h/J47qcbT5T99zwrK4vx9BLen97lsOF4ZmRkmNfSuXPn09435IOdtWvXmjeBvgEAAEBw0OBNg51OnTqd9r4hH+wAAAB7I2cHAADYGsEOAACwNYIdAABgawQ7AADA1gh2AACArRHsAAAAWyPYAQAAtkawAwAAbI1gBwAA2BrBDgAAsDWCHQAAYGsEOwAAwNYIdlz066+/ypAhQ6Rjx47Sp08feemllyQrK8vjxxw+fFgmTpwoPXr0MDu3jh49Wnbu3Cl256vxXLNmjdx0003SrVs3c7nllltkw4YNYne+Gk9n//3vf6VNmzby0UcfSSjw1ZgmJyfLo48+Kj179jS/83r/5cuXi935ajw3bdokI0eOlHPPPVd69eol99xzjxw8eFDszp3xtOzfv9+M1y+//CKF2fYzSXc9R+nWrVvnaNu2rePOO+90/Pjjj45p06Y52rRp43jiiSc8ekxWVpZj8ODBjr59+zrmz5/v+OqrrxyXXHKJo0+fPo7jx4877MpX47lhwwZzn5tuusnx3XffORYvXuwYNmyYOfa///3PYVe+Gk9nqampjosvvtjRokULx4cffuiwO1/+zl9zzTWOnj17Oj777DPHTz/95Bg5cqR5XEJCgsOufDWe+/fvd5x77rmOQYMGmd/3BQsWOPr16+e4/PLLHenp6Q67cmc8LXv37jWfM/q7/PPPPzuc2fkziWDHBTfffLPjyiuvdOTk5OQfe/fddx3x8fGOffv2uf2YL7/80rzh9EPa+Ze3Xbt2jjfffNNhV74az3/+85/mHzrnf+ROnjzp6Natm+Oee+5x2JWvxtPZY489Zv7BC5Vgx1djqgGOfu/8O5+Wlubo37+/+cCyK1+N5+zZs817cseOHfn30QBSj/3yyy8Ou3JnPLOzsx2ffPKJCQ71UlywY+fPJJaxTiMjI0NWrFghF110kYSFheUfv+SSSyQ7O1uWLl3q9mP0ukGDBhIfH59/n7p160qXLl1kyZIlYke+HM/WrVvLzTffLNHR0fn3iY2NlTPPPFMOHDggduTL8bQsW7ZMPv74Y7P0Egp8OaaLFi0yv9/Ov/MVKlQwx2+77TaxI1+OZ3p6urmuXLly/n2qV69uro8cOSJ25M54Wst9//rXv2TQoEHyzDPPSHHs/JlEsHMau3btkszMTGnSpEmB42eccYZUrFhRtm7d6vZj9LrwfVSjRo2KPa8d+HI8NT/n+uuvL3CfHTt2yObNm6V58+ZiR74cT3Xs2DF54IEHZNy4cRIXFyehwJdjunHjRjOOM2fOlAsuuMAE6IMHD5aVK1eKXflyPAcOHCh16tSRSZMmmT9o9HH6QV67dm2Tv2NH7oynOuuss+Tbb7+V+++/39yvOHb+TCLYOY3jx48X+cvBUqlSJTl58qTbj9H7lXSfEydOiB35cjwLS01Nlfvuu8/M9IwYMULsyNfj+dhjj0m9evVMIBkqfDmmmvypHzhz5swxibSvv/66mX20cyK9L8dTP+B1xvGHH36Q3r17y4UXXmhmMN566y2pWrWq2JG7/x5Wr17dzHKf7tx2/Uwi2DmNnJwcnz1Gc6ZK4jw9aSe+HM/CFS+33nqrrF+/Xp577jk5++yzxY58OZ5ffvmlfP/99/L0009LeHjo/FPhyzHVv8j1vfn222/LgAEDTBXNtGnTzAeMXtuRL8dz/vz5MnbsWDOO06dPl9dee83M4mrwuGXLFrEjd8bTVXb+TIos7ycQ6Ky/DoqLlvVYlSpV3H6MXhd3H42gizuvHfhyPC1aJjlq1CjZu3evKcfU5QK78tV4ammqLg2MHz9e6tevb0parX9k9VpzAyIiIsSOfPke1b+QdUlAlxQsGuhoiW9CQoLYkS/H85VXXpEOHTqY33Prw1hL+nV56/nnnzfBj924M56usvNnUuj8ueamhg0bmn/UC/cZ0A+DtLQ0adasmduP0bVRzSkpTB9X3HntwJfjafWCueaaa0xy4nvvvWemte3MV+P5888/m3ydyZMnm946etGESPXII4/kf21HvnyPaqCjCaaFaTCpicp25Mvx3LNnjwkUnWcdNB+lbdu2tp3ZcWc8XWXnzySCndPQfA9tvvTNN98UmD78+uuvJTIyUrp37+72YzSBLjEx0awxWzTJbvXq1bZNrvPleG7bts1UY2kOxNy5c80/gnbnq/Hs16+fqcByvmh+ibr99tvzv7YjX75Hdbnlzz//NBeLBpVr166Vc845R+zIl+PZtGlT00jUeflFK7T+97//maoiO3JnPF1l68+k8q59DwYrVqxwtGrVyjFmzBjHkiVL8hs4Pf744+Z27euydu1ax19//eXyY6zHacOm3r17m/4bX3/9dX4Dp2PHjjnsylfjed1115n76Fjq450vmzdvdtiVr8azsF27doVMnx1fjenRo0dNwzbtBzVv3jzT/PLqq692dOnSxbF7926HXflqPLWRYMuWLR3jxo0z/XW+/fZbx/XXX2/ut2rVKodduTOezn799ddi++zY+TOJYMdF+o+SNnHSN5T+j3/ppZdMt0nnD4GpU6e6/BiLNoDSLpidO3c2/+CNHj26QIMsu/L2eCYlJZnHlHS59tprHXbmq/dnqAY7vhxT/QDSBphdu3Z1dOzY0TSI27Rpk8PufDWe2kF4yJAhpvFd9+7dTUdqO3dM92Q8Txfs2PkzKUz/U96zSwAAAL5Czg4AALA1gh0AAGBrBDsAAMDWCHYAAICtEewAAABbI9gBAAC2RrADAABsjY1AAZTJxIkT5bPPPpPvvvuuxN3kP/30U7n//vsLHNOd06tVqybt27eXO+64w2zgaNm9e3eRDVt1v6OYmBjzM3SPs5EjR5qNNC3/93//Z/ZGOh3dFXvcuHEuPe+WLVuaVvwzZ84scptuQTBnzhxZuXKl2YdIW/PrfkGXXHKJDB06tNi9rTZu3ChvvfWWeYzu16abKeq+Tddee22R12uN2ZNPPilXXXXVaV8XANcR7ADwmSFDhkiXLl3M15mZmZKUlGQCjuuvv94EDrrBqDPdH0o3clXa7/T48eNm3yjdi2vhwoXywQcfSPXq1c3tDzzwQIEdmr/99ltzGT16tNkzyTmA8dS0adPkxRdfNLuVX3rppWYzxtTUVLNhqgYnn3/+udl41npuavny5XLrrbeawEqDm7p168qhQ4dkwYIFMmbMGLPH2F133eXxcwNwegQ7AHymY8eOcuWVVxY49re//c3MymgAMXXq1AK36eaNhe9/ww03yMUXXyx33nmnPPTQQ/LKK6+Y44V3tNedmTXYOe+886Rbt25eew0ayDz//PMyaNAgefzxx81GjJbhw4fLV199JePHj5dnn31W/v3vf+ff9thjj5ldznXGxnnWRwOgG2+80bx+PWfjxo299lwBFI+cHQB+VadOHTPzsmXLFpcfo8GOBkGLFy8u0+M8lZaWZmZuNAibNGlSgUDHMnDgQLnoootk3rx5cuLECXPs8OHDsn37drMkVnh5KyIiwgRwOnO1bt06v70WIJQR7ADwq6ysLDlw4IBZCiqLwYMHmwDhxx9/FH9ZsmSJHD16VG655ZZic3Isd999t5kBqly5svlec400qNHH//XXX0Xur7NSf/zxh5nZAeB7LGMB8JmUlBQzy6FycnJMzo4m7GoujuaslEWLFi3MdUJCgkfPKTk5Of85nc6KFSvMdffu3Uu9X5MmTQp8r8HOFVdcYfKT+vfvL+eff7706tXLzPRoUrMma+sFgH8Q7ADwGc1x0Utho0aNMlVZZaGVXEpnWjyhM0Su2rdvn7k+88wzCxzPzs6WY8eOFbm/zuxYS13/+te/zEzUF198YZbf9KLq1atnAqHbbrutQHUZAN8h2AHgM7r8ozMa1syOll//8MMPJjlXK5OcE3pdWf6yStI9oYnEtWvXLva2m266qcD3+pyVBi3Otm7dKpdffnmRxzuXjevsztNPP23K7LWSTCu3tLJs79698sYbb5hj//nPf6RWrVoevR4Ap0ewA8Bn4uLiTHWUMw0SdAnn448/NmXmzv12SmPN6NSsWdOj59S5c+cS++wUdsYZZ5jrgwcPmsoqiz7+3Xffzf9e82+mTJlS7Dk0N0lncfSiCc/Lli2Tl19+2fTg0coynQEC4FssGgPwO23Ep3Smw1UbNmww161btxZ/0cDI6pnjLDY21gRx1qVVq1YFbtfEZJ21sqqzLBUrVjTJybNmzTINBletWuWHVwGAYAeA31nLQmVZktJmfHp/7dHjLxqYaF7N7NmzJSMjo0yB2YwZM0oMZjTQ0QaFutQFwPcIdgD43ZdffmmuXW3+pzMlGuxoTxvteeMvmnD84IMPyp9//in33Xef6ZpcmG5ZMX369ALHLrvsMlN6/txzzxVb+aVB0ObNm4s0RgTgG+TsAHDLCy+8UGw1UadOnfK/1qZ5+qHvXIquXY41WVebBBZe/tm1a5epXrJmf7RMXJe6Fi1aJM2bN5dHHnlE/E07Puvz0Jyc3377zSzBafm4zvSsXr1avv/+e/O1Vlj169cvP09Hn6t2Udb7a/CjpfOa8Pzf//5X5s+fb7bR0E7KAHyPYAeAR7MzhekHv/aTUXPnzjUX51wX7Umjm3Lq/ljFzXg4L/1Y99eNPEeMGGG+Lw9apdW7d2/58MMPTaCmydWqfv36Zv+vq6++usgeXLofluYX6aaiWoGmj9HEbH09EyZMkOuuu06ioqLK5fUAoSbMUbimEgAAwEbI2QEAALZGsAMAAGyNYAcAANgawQ4AALA1gh0AAGBrBDsAAMDWCHYAAICtEewAAABbI9gBAAC2RrADAABsjWAHAADYGsEOAACwNYIdAAAgdvb/8RUgy/Gu5WgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Assume your model is sklearn-compatible (or wrapped)\n",
    "features = [\"LBDTHGSI\"]\n",
    "PartialDependenceDisplay.from_estimator(best_model, X_test_sel, features)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(contamination=0.1, random_state=42)\n",
    "df[\"outlier\"] = model.fit_predict(df[[\"value\"]])\n",
    "\n",
    "outliers = df[df[\"outlier\"] == -1]\n",
    "print(\"Outliers by Isolation Forest:\\n\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b32a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBXTST</th>\n",
       "      <th>LBDBSESI</th>\n",
       "      <th>RIDEXPRG</th>\n",
       "      <th>LBDTHGSI</th>\n",
       "      <th>RHQ160</th>\n",
       "      <th>LBDBCDSI</th>\n",
       "      <th>LBDBPBSI</th>\n",
       "      <th>LBDBMNSI</th>\n",
       "      <th>RIDAGEMN</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RHQ131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367.00</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.043</td>\n",
       "      <td>142.52</td>\n",
       "      <td>744.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505.00</td>\n",
       "      <td>2.53</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.126</td>\n",
       "      <td>130.69</td>\n",
       "      <td>636.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.00</td>\n",
       "      <td>2.90</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.089</td>\n",
       "      <td>112.49</td>\n",
       "      <td>936.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.50</td>\n",
       "      <td>2.47</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.024</td>\n",
       "      <td>228.80</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>543.00</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.035</td>\n",
       "      <td>132.33</td>\n",
       "      <td>264.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.70</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.017</td>\n",
       "      <td>161.63</td>\n",
       "      <td>384.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>381.00</td>\n",
       "      <td>2.77</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.029</td>\n",
       "      <td>175.65</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>685.00</td>\n",
       "      <td>2.61</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.095</td>\n",
       "      <td>61.89</td>\n",
       "      <td>672.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>336.00</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>8.63</td>\n",
       "      <td>0.075</td>\n",
       "      <td>123.41</td>\n",
       "      <td>552.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.90</td>\n",
       "      <td>2.22</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.009</td>\n",
       "      <td>276.85</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>487.00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.023</td>\n",
       "      <td>132.87</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.54</td>\n",
       "      <td>3.27</td>\n",
       "      <td>203.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.109</td>\n",
       "      <td>176.74</td>\n",
       "      <td>684.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50.80</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>504.0</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0.087</td>\n",
       "      <td>261.20</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>569.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.021</td>\n",
       "      <td>195.13</td>\n",
       "      <td>168.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.80</td>\n",
       "      <td>2.32</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.020</td>\n",
       "      <td>189.66</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.90</td>\n",
       "      <td>2.98</td>\n",
       "      <td>203.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.044</td>\n",
       "      <td>177.83</td>\n",
       "      <td>960.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>310.00</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.026</td>\n",
       "      <td>160.18</td>\n",
       "      <td>672.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>392.00</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.023</td>\n",
       "      <td>160.54</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>468.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.013</td>\n",
       "      <td>120.13</td>\n",
       "      <td>264.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.97</td>\n",
       "      <td>2.42</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.094</td>\n",
       "      <td>149.44</td>\n",
       "      <td>816.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LBXTST  LBDBSESI  RIDEXPRG  LBDTHGSI  RHQ160  LBDBCDSI  LBDBPBSI  \\\n",
       "0   367.00      2.57    -300.0       2.3  -300.0      1.78     0.043   \n",
       "1   505.00      2.53    -300.0      15.4  -300.0     31.41     0.126   \n",
       "2   104.00      2.90    -300.0       3.5  -300.0      3.83     0.089   \n",
       "3    19.50      2.47     202.0       2.3     NaN      1.25     0.024   \n",
       "4   543.00      2.42    -300.0       6.9  -300.0      1.78     0.035   \n",
       "5    30.70      2.50       2.0       7.7     3.0      2.22     0.017   \n",
       "6   381.00      2.77    -300.0      21.5  -300.0      1.07     0.029   \n",
       "7   685.00      2.61    -300.0      17.7  -300.0      2.76     0.095   \n",
       "8   336.00      2.48    -300.0       2.7  -300.0      8.63     0.075   \n",
       "9    20.90      2.22     202.0       1.0     NaN      1.87     0.009   \n",
       "10  487.00      2.96    -300.0       1.8  -300.0      0.98     0.023   \n",
       "11    9.54      3.27     203.0       2.8     3.0      1.60     0.109   \n",
       "12   50.80      2.42       2.0      16.1   504.0      9.88     0.087   \n",
       "13  569.00      1.99    -300.0       1.6  -300.0      0.98     0.021   \n",
       "14   20.80      2.32     202.0       2.3     NaN      1.78     0.020   \n",
       "15   19.90      2.98     203.0       3.5     4.0      5.43     0.044   \n",
       "16  310.00      3.05    -300.0       8.8  -300.0      2.05     0.026   \n",
       "17  392.00      2.48    -300.0      12.8  -300.0      0.89     0.023   \n",
       "18  468.00      2.45    -300.0       4.9  -300.0      1.25     0.013   \n",
       "19    6.97      2.42     203.0       1.0     4.0      2.58     0.094   \n",
       "\n",
       "    LBDBMNSI  RIDAGEMN  RIAGENDR  RHQ131  \n",
       "0     142.52     744.0         1  -300.0  \n",
       "1     130.69     636.0         1  -300.0  \n",
       "2     112.49     936.0         1  -300.0  \n",
       "3     228.80     132.0         2     NaN  \n",
       "4     132.33     264.0         1  -300.0  \n",
       "5     161.63     384.0         2     1.0  \n",
       "6     175.65     216.0         1  -300.0  \n",
       "7      61.89     672.0         1  -300.0  \n",
       "8     123.41     552.0         1  -300.0  \n",
       "9     276.85     192.0         2     NaN  \n",
       "10    132.87     180.0         1  -300.0  \n",
       "11    176.74     684.0         2     1.0  \n",
       "12    261.20     288.0         2     2.0  \n",
       "13    195.13     168.0         1  -300.0  \n",
       "14    189.66     180.0         2     NaN  \n",
       "15    177.83     960.0         2     1.0  \n",
       "16    160.18     672.0         1  -300.0  \n",
       "17    160.54     204.0         1  -300.0  \n",
       "18    120.13     264.0         1  -300.0  \n",
       "19    149.44     816.0         2     1.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c74d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction for new input: 185.96628\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"./models/LBXTST/xgboost_model_1.joblib\"\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# Example new input (dummy values just for illustration)\n",
    "new_data = pd.DataFrame([{\n",
    "    'LBDBSESI': 2.42,\n",
    "    'RIDEXPRG': 2.0,\n",
    "    'LBDTHGSI': 16.1,\n",
    "    'RHQ160': 504.0,\n",
    "    'LBDBCDSI': 9.88,\n",
    "    'LBDBPBSI': 0.087,\n",
    "    'LBDBMNSI': 261.20,\n",
    "    'RIDAGEMN': 288.0\t,\n",
    "    'RIAGENDR': 2\n",
    "    # ⚠️ must include all selected features that your model expects\n",
    "}])\n",
    "\n",
    "# Reindex to ensure same columns order as training\n",
    "new_data = new_data.reindex(columns=results_dic[\"selected_features\"], fill_value=0)\n",
    "\n",
    "# Predict\n",
    "prediction = loaded_model.predict(new_data)\n",
    "print(\"✅ Prediction for new input:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_domain_rules(df):\n",
    "    df = df.copy()\n",
    "    is_male = df[\"RIAGENDR\"] == 1\n",
    "    is_female = df[\"RIAGENDR\"] == 2\n",
    "    age = df[\"RIDAGEMN\"]\n",
    "\n",
    "    # RIDEXPRG logic\n",
    "    df.loc[is_male, \"RIDEXPRG\"] = -1000\n",
    "    df.loc[is_female & (age < 20), \"RIDEXPRG\"] = 202\n",
    "    df.loc[is_female & (age > 44), \"RIDEXPRG\"] = 203\n",
    "\n",
    "    # Replace NaNs in RHQ/RHD cols for males\n",
    "    df = mark_male_nans(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn  # for sklearn models\n",
    "\n",
    "def preprocess_and_model_mlflow(\n",
    "    df,\n",
    "    target_col,\n",
    "    model,\n",
    "    param_grid,\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    top_features=60,\n",
    "    run_shap=True,\n",
    "    experiment_name=\"HormonePrediction\"\n",
    "):\n",
    "    # Apply domain preprocessing rules\n",
    "    df = preprocess_domain_rules(df)\n",
    "\n",
    "    # Start an MLflow run\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run():\n",
    "        # Run your pipeline\n",
    "        current_result, results_dict = preprocess_and_model_shap(\n",
    "            df=df,\n",
    "            target_col=target_col,\n",
    "            model=model,\n",
    "            param_grid=param_grid,\n",
    "            drop_cols=drop_cols,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            top_features=top_features,\n",
    "            run_shap=run_shap,\n",
    "        )\n",
    "\n",
    "        # ---------------- Log params ----------------\n",
    "        mlflow.log_params(results_dict[\"best_params\"])\n",
    "        mlflow.log_param(\"target_col\", target_col)\n",
    "        mlflow.log_param(\"selected_features\", results_dict[\"selected_features\"])\n",
    "\n",
    "        # ---------------- Log metrics ----------------\n",
    "        mlflow.log_metrics(results_dict[\"eval_results\"])\n",
    "\n",
    "        # ---------------- Log SHAP artifacts ----------------\n",
    "        if run_shap:\n",
    "            mlflow.log_artifacts(\"shap_outputs\")\n",
    "\n",
    "        # ---------------- Log model ----------------\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=results_dict[\"best_model\"],\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=\"HormonePredictor\"\n",
    "        )\n",
    "\n",
    "        print(\"✅ Run logged in MLflow\")\n",
    "\n",
    "    return current_result, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2179b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/18 23:30:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/18 23:30:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run logged in MLflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'HormonePredictor' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'HormonePredictor'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid={\n",
    "        \"n_estimators\": [20,50, 100, 500, 1000],\n",
    "        \"max_depth\": [3, 5, 7, 12],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.001, 0.01]\n",
    "    }\n",
    "\n",
    "curr, res = preprocess_and_model_mlflow(\n",
    "    df=df_filtered,\n",
    "    target_col=\"LBXTST\",   # example hormone level\n",
    "    model=XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "    param_grid=param_grid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f2e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///d:/UOM/Sem%2005/DS%20PROJECT/Project_notebooks/mlruns\n"
     ]
    }
   ],
   "source": [
    "print(mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249fa394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "\n",
    "input_example = res['X_test'][5:]\n",
    "signature = mlflow.models.signature.infer_signature(X_test, preds)\n",
    "\n",
    "mlflow.xgboost.log_model(\n",
    "    xgb_model,\n",
    "    artifact_path=\"model\",\n",
    "    input_example=input_example,\n",
    "    signature=signature\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
